{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"FNN - model_train_test 2HL.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"mount_file_id":"14AiIqaruvB6yB4zKHLMdPp6MAWSCF4Sw","authorship_tag":"ABX9TyMn/r+nqCrkiuRgMmHDeHv2"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"1nGofDqiwTwX","colab_type":"text"},"source":["## FNN Structure"]},{"cell_type":"code","metadata":{"id":"60klT_XixqI_","colab_type":"code","colab":{}},"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","torch.manual_seed(777)\n","\n","class Net(nn.Module):\n","\t'''\n","\tThis Class Defines the Structure of the Artificial Neural Network\n","\t'''\n","\tdef __init__(self, HN1, HN2):\t\n","\t\tself.HN1 = HN1\t\t\t\n","\t\tself.HN2 = HN2\n","\t\tsuper().__init__()                                                             # Run the intitialision method from base class nn.module.\n","\t\tself.fc1 = nn.Linear(5, self.HN1)                                                    # Define the first fully connected layer. nn.Linear simply connects the input nodes to the output nodes in the standard way. The input layer contains 5 nodes. The output layer (first hidden layer), consists of 15 nodes.\n","\t\tself.fc2 = nn.Linear(self.HN1, self.HN2)                                                   # Hidden layer 2: each node takes in 15 values, contains 15 nodes hence outputs 15 values.\n","\t\tself.fc3 = nn.Linear(self.HN2, 4)                                                    # Output Layer: each node takes in 15 values, contain 3 nodes (one for each rate of change: X, N and Lu) hence outputs 3 values.\n","\n","\tdef forward(self, x):                                                              # This method feeds data into the network and propagates it forward.\n","\t\tx = torch.sigmoid(self.fc1(x))                                                 # Feed the dataset, x, through fc1 and apply the Sigmoid activation function to the weighted sum of each neuron. Then assign the transformed dataset to x. Next, feed the transformed dataset through fc2 and so on... until we reach the output layer. The activation fucntion basically decides if the neuron is 'firing' like real neurones in the human brain. The activation function prevents massive output numbers.\n","\t\tx = torch.sigmoid(self.fc2(x))\n","\t\tx = self.fc3(x)\n","\t\treturn x "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KgxebdsQwYN2","colab_type":"text"},"source":["## Data Replication"]},{"cell_type":"code","metadata":{"id":"rmKIce7Ex0nr","colab_type":"code","colab":{}},"source":["import numpy as np\n","import pandas as pd\n","\n","## Create a function which takes in a dataset and replicates it\n","def replicate_data(data, replications, noise):                                                      # Create a function which accepts a dataset and replicates it\n","    cols = list(data.columns) \n","    dataR = data[cols[0:4]]                                                                         # Create a secondary dataframe containing only columns 1-3 (the columns we want to replicate)\n","    df = data                                                                                       # Create the output dataframe that will contain both the original and the replicated data\n","    new_data = pd.DataFrame(columns=data.columns)\n","    i = 0                                                                                           # Initialise replication counter to 0\n","    while i < replications:\n","        replicated_data =  np.random.uniform(dataR-dataR*noise, dataR+dataR*noise)                  # Create random noise for each value in columns 2-4 of dataset\n","        replicated_data = pd.DataFrame(data=replicated_data, index=None, columns=dataR.columns)      # Cast the replicated data as a pandas DataFrame Object\n","        replicated_data['LI'] = df[cols[4]]                                                          # Add the missing light intensity column back into the replicated_data set\n","        new_data = new_data.append(replicated_data, ignore_index=True, sort=False)\n","        i += 1\n","    return new_data"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Lilt-uMLwjL5","colab_type":"text"},"source":["## Train Function"]},{"cell_type":"code","metadata":{"id":"Nft0rbUzx7sn","colab_type":"code","colab":{}},"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from tqdm import tqdm\n","\n","def train(net, inputs, labels, EPOCHS, l_rate, BATCH_SIZE):\n","\tnet.train()                                                                         \n","\toptimiser = optim.Adam(net.parameters(), lr = l_rate)\t\t\t\t\t\t\t\t\t   # net.parameters(): all of the adjustable parameters in our network. lr: a hyperparameter adjusts the size of the step that the optimizer will take to minimise the loss.\n","\tloss_function = nn.MSELoss(reduction='mean')\n","\tX = torch.Tensor(inputs).view(-1, 5)\n","\ty = torch.Tensor(labels)\n","\tfor epoch in range(EPOCHS):\n","\t\tfor i in tqdm(range(0, len(X), BATCH_SIZE), disable=True):\n","\t\t\tbatch_X = X[i:i+BATCH_SIZE].view(-1, 5)\n","\t\t\tbatch_y = y[i:i+BATCH_SIZE]\n","\t\t\toptimiser.zero_grad()\n","\t\t\toutputs = net(batch_X)\n","\t\t\tloss = loss_function(outputs, batch_y)\n","\t\t\tloss.backward()\n","\t\t\toptimiser.step()\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8LQWGr9nw52i","colab_type":"text"},"source":["## Test Function"]},{"cell_type":"code","metadata":{"id":"41M8U7feyCDG","colab_type":"code","colab":{}},"source":["import torch\n","import numpy as np\n","import pandas as pd\n","\n","def test(test_inputs, test_labels, net):\n","    net.eval()\n","    test_X = torch.Tensor(test_inputs).view(-1, 5)\n","    test_y = torch.Tensor(test_labels)\n","\n","    predictionNumpy = []\n","    with torch.no_grad():\n","        for i in range(0, len(test_X)):\n","            net_out = net(test_X[i].view(-1, 5))\n","            predictionNumpy.append(net_out[0].numpy())              # The output from the net is a tensor which contains only one element which is a list. The list contains the 3 output values. We only want the list, not the tensoor containing one element which is a list.\n","\n","    experimental = []\n","    for data in test_y:\n","        experimental.append(data.numpy())\n","\n","    squared_error_X = []\n","    squared_error_N = []\n","    squared_error_L = [] \n","    squared_error_C = [] \n","\n","    for i in range(0, len(experimental)):\n","            X_error = experimental[i][0] - predictionNumpy[i][0]\n","            N_error = experimental[i][1] - predictionNumpy[i][1]\n","            L_error = experimental[i][2] - predictionNumpy[i][2]\n","            C_error = experimental[i][3] - predictionNumpy[i][3]\n","            squared_error_X.append(X_error**2)\n","            squared_error_N.append(N_error**2)\n","            squared_error_L.append(L_error**2)\n","            squared_error_C.append(C_error**2)\n","\n","    MSE_X1 = sum(squared_error_X[0:14])/14          \n","    MSE_N1 = sum(squared_error_N[0:14])/14\n","    MSE_L1 = sum(squared_error_L[0:14])/14\n","    MSE_C1 = sum(squared_error_L[0:14])/14\n","    \n","    MSE_X2 = sum(squared_error_X[14:28])/14                                     #This code has been designed to be compatible with more than one test data set\n","    MSE_N2 = sum(squared_error_N[14:28])/14\n","    MSE_L2 = sum(squared_error_L[14:28])/14\n","    MSE_C2 = sum(squared_error_C[14:28])/14\n","    MSE_list = [MSE_X1, MSE_N1, MSE_L1, MSE_C1, MSE_X2, MSE_N2, MSE_L2, MSE_C2]\n","    AVG_MSE = sum(MSE_list)/8\n","\n","    LI1, LI2 = test_inputs[0][4], test_inputs[14][4]\n","    predictions_online = []\n","    for index, value in enumerate(test_inputs):\n","        B = value[0] + predictionNumpy[index][0]\n","        N = value[1] + predictionNumpy[index][1]\n","        F = value[2] + predictionNumpy[index][2]\n","        NIC = value[3] + predictionNumpy[index][3]\n","\n","        if index < 14:\n","            predictions_online.append([B, N, F, NIC, LI1])\n","\n","        if index >= 14:\n","            predictions_online.append([B, N, F, NIC, LI2])\n","\n","    predictions_offline = []\n","    B1, B2 = test_inputs[0][0], test_inputs[14][0]\n","    N1, N2 = test_inputs[0][1], test_inputs[14][1]\n","    F1, F2 = test_inputs[0][2], test_inputs[14][2]\n","    NIC1, NIC2 = test_inputs[0][3], test_inputs[14][3]\n","    for index, value in enumerate(test_inputs):\n","        if index < 14:\n","            net_out = net(torch.Tensor([B1, N1, F1, NIC1, LI1]))\n","            B = B1 + net_out[0]   \n","            N = N1 + net_out[1]\n","            F = F1 + net_out[2]\n","            NIC = NIC1 + net_out[3]\n","            predictions_offline.append([float(B), float(N), float(F), float(NIC), float(LI1)])\n","            B1 = B\n","            N1 = N\n","            F1 = F\n","            NIC1 = NIC\n","        \n","        if index >= 14:\n","            net_out = net(torch.Tensor([B2, N2, F2, NIC2, LI2]))\n","            B = B2 + net_out[0] \n","            N = N2 + net_out[1] \n","            F = F2 + net_out[2] \n","            NIC = NIC2 + net_out[3] \n","            predictions_offline.append([float(B), float(N), float(F), float(NIC), float(LI2)])\n","            B2 = B\n","            N2 = N\n","            F2 = F\n","            NIC2 = NIC\n","            \n","    return AVG_MSE, predictions_online, predictions_offline\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kahLjq2Hzvc1","colab_type":"text"},"source":["## Datasets Import\n"]},{"cell_type":"code","metadata":{"id":"yT7bYF8dl9dL","colab_type":"code","colab":{}},"source":["# Clone the entire repo.\n","!git clone -l -s git://github.com/Arymega/FAME_Bioprocess_Simulation_with_RNN_and_FNN.git cloned-repo\n","%cd cloned-repo\n","!ls"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"89RuOgc6z4Pa","colab_type":"text"},"source":["## Perform Training"]},{"cell_type":"code","metadata":{"id":"ICsyiYZRxOT9","colab_type":"code","colab":{}},"source":["import torch\n","import pandas as pd\n","import numpy as np \n","import os\n","import time\n","start_time = time.time()\n","from sklearn.preprocessing import StandardScaler\n","\n","# Load training and testing data as pd dataframe\n","training_data = pd.read_excel('/content/cloned-repo/Common Files/Datasets.xlsx', sheet_name='Train')\n","testing_data = pd.read_excel('/content/cloned-repo/Common Files/Datasets.xlsx', sheet_name='Test')\n","\n","# Standardise training and testing data\n","scaler_train = StandardScaler()\n","scaler_test = StandardScaler()\n","\n","scaler_train.fit(training_data)\n","scaler_test.fit(testing_data)\n","\n","testing_data = scaler_test.transform(testing_data)\n","\n","# Convert training data to pd dataframe\n","columns = \"B N F NIC LI\".split()\n","training_data = pd.DataFrame(data=training_data, index=None, columns=columns)\n","\n","# Replicate the training data\n","replicated_data1 = replicate_data(training_data, 50, 0.03)\n","replicated_data2 = replicate_data(training_data, 50, 0.05)\n","\n","training_data = training_data.append(replicated_data1, ignore_index=True, sort=False)\n","training_data = training_data.append(replicated_data2, ignore_index=True, sort=False)\n","\n","training_data = scaler_train.transform(training_data)\n","training_data = np.array(training_data)\n","\n","# Calculate training and testing labels\n","try:\n","    a = []\n","    for index, row in enumerate(training_data):\n","        dB = training_data[index + 1][0] - row[0]\n","        dN = training_data[index + 1][1] - row[1]\n","        dF = training_data[index + 1][2] - row[2]\n","        dNIC = training_data[index + 1][3] - row[3]\n","\n","        rates = [dB, dN, dF, dNIC]\n","        a.append(rates)\n","except IndexError:\n","    rates = [0, 0, 0, 0]\n","    a.append(rates)\n","\n","a = np.array(a)\n","training_data = np.append(training_data, a, axis=1)\n","\n","try:\n","    a = []\n","    for index, row in enumerate(testing_data):\n","        dB = testing_data[index + 1][0] - row[0]\n","        dN = testing_data[index + 1][1] - row[1]\n","        dF = testing_data[index + 1][2] - row[2]\n","        dNIC = testing_data[index + 1][3] - row[3]\n","\n","        rates = [dB, dN, dF, dNIC]\n","        a.append(rates)\n","except IndexError:\n","    rates = [0, 0, 0, 0]\n","    a.append(rates)\n","\n","a = np.array(a)\n","testing_data = np.append(testing_data, a, axis=1)\n","\n","# Remove 15th datapoints from all corresponding training and testing sets\n","count = 0\n","decrement = 0\n","for index, row in enumerate(training_data):\n","    count += 1\n","    if count == 15:\n","        delete = index - decrement\n","        training_data = np.delete(training_data, delete, 0)\n","        decrement += 1\n","        count = 0\n","\n","count = 0\n","decrement = 0\n","for index, row in enumerate(testing_data):\n","    count += 1\n","    if count == 15:\n","        delete = index - decrement\n","        testing_data = np.delete(testing_data, delete, 0)\n","        decrement += 1\n","        count = 0\n","\n","# Shuffle training data\n","np.random.shuffle(training_data)\n","\n","# Define structure of optimal network\n","HL = 2\n","HN1, HN2 = 14, 16\n","EPOCHS = 176\n","BATCH_SIZE = 100\n","LR = 0.0003\n","\n","xcl_dir = '/content/drive/My Drive/Colab Notebooks/GitHub/MSc/FNN/Results/2HL/' #create a new folder for prediction results\n","try:\n","  os.mkdir(xcl_dir)\n","except:\n","  pass\n","\n","\n","# Instantiate the network and prepare data\n","for count in range(1):\n","  avg_mse=10\n","  min_mse=10\n","\n","  while count < 50:\n","    net = Net(HN1, HN2)\n","    training_inputs = training_data[:, 0:5]\n","    training_labels = training_data[:, 5:]\n","    test_inputs = testing_data[:, 0:5]\n","    test_labels = testing_data[:, 5:]\n","\n","    # Train and test the network\n","    train(net, training_inputs, training_labels, EPOCHS, LR, BATCH_SIZE)\n","    avg_mse, predictions_online, predictions_offline = test(test_inputs, test_labels, net)\n","   \n","    count = count + 1\n","    if min_mse >= avg_mse or count==5 : #count=* is related to while count above\n","      min_mse = avg_mse\n","      count_min = count\n","      # Save file every minimum found\n","\n","      predictions_online_inverse_transform = scaler_test.inverse_transform(predictions_online)\n","      predictions_offline_inverse_transform = scaler_test.inverse_transform(predictions_offline)\n","\n","      online = pd.DataFrame(predictions_online_inverse_transform)\n","      offline = pd.DataFrame(predictions_offline_inverse_transform)\n","      avg_mse = pd.DataFrame([avg_mse, 0])\n","      f= round(min_mse.item(), 5)\n","\n","      with pd.ExcelWriter('{xcl_dir}Predictions {count}_{f}_{x}_{y}-{z}_{a}_{b}_{c}.xlsx'.format(xcl_dir=xcl_dir, x=HL, y=HN1, z=HN2, a=EPOCHS, b=LR, c=BATCH_SIZE, count=count, f=f)) as writer:  \n","          offline.to_excel(writer, sheet_name='Offline', startrow=1, startcol=1)\n","          online.to_excel(writer, sheet_name='Online', startrow=1, startcol=1)\n","          avg_mse.to_excel(writer, sheet_name='Avg_MSE', startrow=1, startcol=1)\n","      torch.save(net.state_dict(), '{xcl_dir}Model {count}_{f}_{x}_{y}-{z}_{a}_{b}_{c}.pt'.format(xcl_dir=xcl_dir, x=HL, y=HN1, z=HN2, a=EPOCHS, b=LR, c=BATCH_SIZE, count=count, f=f))\n","    print(avg_mse, min_mse, count, count_min)\n","\n","print(f'\\nDuration: {time.time() - start_time:.0f} seconds')\n"],"execution_count":null,"outputs":[]}]}