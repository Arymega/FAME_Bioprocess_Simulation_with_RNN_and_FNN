{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"FNN - model_train_test V1.1 - 2HL - run5.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1293d-Dg-iWHayPjg0PzwDyTL0TsumZ-g","authorship_tag":"ABX9TyNR+qFPUISlmG11bsFEpmGB"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"60klT_XixqI_","colab_type":"code","colab":{}},"source":["#from ann2 import Net\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","torch.manual_seed(777)\n","\n","class Net(nn.Module):\n","\t'''\n","\tThis Class Defines the Structure of the Artificial Neural Network\n","\t'''\n","\tdef __init__(self, HN1, HN2):\t\n","\t\tself.HN1 = HN1\t\t\t\n","\t\tself.HN2 = HN2\n","\t\tsuper().__init__()                                                             # Run the intitialision method from base class nn.module.\n","\t\tself.fc1 = nn.Linear(5, self.HN1)                                                    # Define the first fully connected layer. nn.Linear simply connects the input nodes to the output nodes in the standard way. The input layer contains 5 nodes. The output layer (first hidden layer), consists of 15 nodes.\n","\t\tself.fc2 = nn.Linear(self.HN1, self.HN2)                                                   # Hidden layer 2: each node takes in 15 values, contains 15 nodes hence outputs 15 values.\n","\t\tself.fc3 = nn.Linear(self.HN2, 4)                                                    # Output Layer: each node takes in 15 values, contain 3 nodes (one for each rate of change: X, N and Lu) hence outputs 3 values.\n","\n","\tdef forward(self, x):                                                              # This method feeds data into the network and propagates it forward.\n","\t\tx = torch.sigmoid(self.fc1(x))                                                 # Feed the dataset, x, through fc1 and apply the Sigmoid activation function to the weighted sum of each neuron. Then assign the transformed dataset to x. Next, feed the transformed dataset through fc2 and so on... until we reach the output layer. The activation fucntion basically decides if the neuron is 'firing' like real neurones in the human brain. The activation function prevents massive output numbers.\n","\t\tx = torch.sigmoid(self.fc2(x))\n","\t\tx = self.fc3(x)\n","\t\treturn x \n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rmKIce7Ex0nr","colab_type":"code","colab":{}},"source":["#from replicate import replicate_data \n","\n","import numpy as np\n","import pandas as pd\n","\n","## Create a function which takes in a dataset and replicates it\n","def replicate_data(data, replications, noise):                                                      # Create a function which accepts a dataset and replicates it\n","    cols = list(data.columns) \n","    dataR = data[cols[0:4]]                                                                         # Create a secondary dataframe containing only columns 1-3 (the columns we want to replicate)\n","    df = data                                                                                       # Create the output dataframe that will contain both the original and the replicated data\n","    new_data = pd.DataFrame(columns=data.columns)\n","    i = 0                                                                                           # Initialise replication counter to 0\n","    while i < replications:\n","        replicated_data =  np.random.uniform(dataR-dataR*noise, dataR+dataR*noise)                  # Create random noise for each value in columns 2-4 of dataset\n","        replicated_data = pd.DataFrame(data=replicated_data, index=None, columns=dataR.columns)      # Cast the replicated data as a pandas DataFrame Object\n","        replicated_data['LI'] = df[cols[4]]                                                          # Add the missing light intensity column back into the replicated_data set\n","        #replicated_data['NIC'] = df[cols[4]]                                                         # Add the missing nitrate inflow concentration back into the replicated_data set\n","        new_data = new_data.append(replicated_data, ignore_index=True, sort=False)\n","        i += 1\n","    return new_data\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Nft0rbUzx7sn","colab_type":"code","colab":{}},"source":["#from train import train\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from tqdm import tqdm\n","\n","def train(net, inputs, labels, EPOCHS, l_rate, BATCH_SIZE):\n","\tnet.train()                                                                         \n","\toptimiser = optim.Adam(net.parameters(), lr = l_rate)\t\t\t\t\t\t\t\t\t   # net.parameters(): all of the adjustable parameters in our network. lr: a hyperparameter adjusts the size of the step that the optimizer will take to minimise the loss.\n","\tloss_function = nn.MSELoss(reduction='mean')\n","\n","\tX = torch.Tensor(inputs).view(-1, 5)\n","\ty = torch.Tensor(labels)\n","\n","\tfor epoch in range(EPOCHS):\n","\t\tfor i in tqdm(range(0, len(X), BATCH_SIZE), disable=True):\n","\t\t\tbatch_X = X[i:i+BATCH_SIZE].view(-1, 5)\n","\t\t\tbatch_y = y[i:i+BATCH_SIZE]\n","\n","\t\t\toptimiser.zero_grad()\n","\t\t\toutputs = net(batch_X)\n","\t\t\tloss = loss_function(outputs, batch_y)\n","\t\t\tloss.backward()\n","\t\t\toptimiser.step()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"41M8U7feyCDG","colab_type":"code","colab":{}},"source":["import torch\n","import numpy as np\n","import pandas as pd\n","\n","\n","def test(test_inputs, test_labels, net):\n","    net.eval()\n","    test_X = torch.Tensor(test_inputs).view(-1, 5)\n","    test_y = torch.Tensor(test_labels)\n","\n","    predictionNumpy = []\n","    with torch.no_grad():\n","        for i in range(0, len(test_X)):\n","            net_out = net(test_X[i].view(-1, 5))\n","            predictionNumpy.append(net_out[0].numpy())              # The output from the net is a tensor which contains only one element which is a list. The list contains the 3 output values. We only want the list, not the tensoor containing one element which is a list.\n","\n","    experimental = []\n","    for data in test_y:\n","        experimental.append(data.numpy())\n","\n","    squared_error_X = []\n","    squared_error_N = []\n","    squared_error_L = [] \n","    squared_error_C = [] \n","\n","    for i in range(0, len(experimental)):\n","            X_error = experimental[i][0] - predictionNumpy[i][0]\n","            N_error = experimental[i][1] - predictionNumpy[i][1]\n","            L_error = experimental[i][2] - predictionNumpy[i][2]\n","            C_error = experimental[i][3] - predictionNumpy[i][3]\n","            squared_error_X.append(X_error**2)\n","            squared_error_N.append(N_error**2)\n","            squared_error_L.append(L_error**2)\n","            squared_error_C.append(C_error**2)\n","\n","    MSE_X1 = sum(squared_error_X[0:14])/14\n","    MSE_N1 = sum(squared_error_N[0:14])/14\n","    MSE_L1 = sum(squared_error_L[0:14])/14\n","    MSE_C1 = sum(squared_error_L[0:14])/14\n","    \n","    MSE_X2 = sum(squared_error_X[14:28])/14\n","    MSE_N2 = sum(squared_error_N[14:28])/14\n","    MSE_L2 = sum(squared_error_L[14:28])/14\n","    MSE_C2 = sum(squared_error_C[14:28])/14\n","    MSE_list = [MSE_X1, MSE_N1, MSE_L1, MSE_C1, MSE_X2, MSE_N2, MSE_L2, MSE_C2]\n","    AVG_MSE = sum(MSE_list)/8\n","\n","\n","    LI1, LI2 = test_inputs[0][4], test_inputs[14][4]\n","   # NIC1, NIC2 = test_inputs[0][4], test_inputs[12][4]\n","    predictions_online = []\n","    for index, value in enumerate(test_inputs):\n","        BC = value[0] + predictionNumpy[index][0]\n","        NC = value[1] + predictionNumpy[index][1]\n","        LP = value[2] + predictionNumpy[index][2]\n","        NIC = value[3] + predictionNumpy[index][3]\n","\n","        if index < 14:\n","            predictions_online.append([BC, NC, LP, NIC, LI1])\n","\n","        if index >= 14:\n","            predictions_online.append([BC, NC, LP, NIC, LI2])\n","\n","    predictions_offline = []\n","    BC1, BC2 = test_inputs[0][0], test_inputs[14][0]\n","    NC1, NC2 = test_inputs[0][1], test_inputs[14][1]\n","    LP1, LP2 = test_inputs[0][2], test_inputs[14][2]\n","    NIC1, NIC2 = test_inputs[0][3], test_inputs[14][3]\n","    for index, value in enumerate(test_inputs):\n","        if index < 14:\n","            net_out = net(torch.Tensor([BC1, NC1, LP1, NIC1, LI1]))\n","            BC = BC1 + net_out[0]   \n","            NC = NC1 + net_out[1]\n","            LP = LP1 + net_out[2]\n","            NIC = NIC1 + net_out[3]\n","            predictions_offline.append([float(BC), float(NC), float(LP), float(NIC), float(LI1)])\n","            BC1 = BC\n","            NC1 = NC\n","            LP1 = LP\n","            NIC1 = NIC\n","        \n","        if index >= 14:\n","            net_out = net(torch.Tensor([BC2, NC2, LP2, NIC2, LI2]))\n","            BC = BC2 + net_out[0] \n","            NC = NC2 + net_out[1] \n","            LP = LP2 + net_out[2] \n","            NIC = NIC2 + net_out[3] \n","            predictions_offline.append([float(BC), float(NC), float(LP), float(NIC), float(LI2)])\n","            BC2 = BC\n","            NC2 = NC\n","            LP2 = LP\n","            NIC2 = NIC\n","            \n","    return AVG_MSE, predictions_online, predictions_offline\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yT7bYF8dl9dL","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":221},"executionInfo":{"status":"ok","timestamp":1597998747372,"user_tz":-60,"elapsed":8660,"user":{"displayName":"Ary Mega","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhprJbEh5igyOrqL87s-U1xxnUAI4rx9VWMM98Llw=s64","userId":"10199364316992018908"}},"outputId":"82afa556-a7cd-4bf1-8cf9-9c76ac4b6418"},"source":["# Clone the entire repo.\n","!git clone -l -s git://github.com/Arymega/RNN2.git cloned-repo\n","%cd cloned-repo\n","!ls"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Cloning into 'cloned-repo'...\n","warning: --local is ignored\n","remote: Enumerating objects: 34, done.\u001b[K\n","remote: Counting objects:   2% (1/34)\u001b[K\rremote: Counting objects:   5% (2/34)\u001b[K\rremote: Counting objects:   8% (3/34)\u001b[K\rremote: Counting objects:  11% (4/34)\u001b[K\rremote: Counting objects:  14% (5/34)\u001b[K\rremote: Counting objects:  17% (6/34)\u001b[K\rremote: Counting objects:  20% (7/34)\u001b[K\rremote: Counting objects:  23% (8/34)\u001b[K\rremote: Counting objects:  26% (9/34)\u001b[K\rremote: Counting objects:  29% (10/34)\u001b[K\rremote: Counting objects:  32% (11/34)\u001b[K\rremote: Counting objects:  35% (12/34)\u001b[K\rremote: Counting objects:  38% (13/34)\u001b[K\rremote: Counting objects:  41% (14/34)\u001b[K\rremote: Counting objects:  44% (15/34)\u001b[K\rremote: Counting objects:  47% (16/34)\u001b[K\rremote: Counting objects:  50% (17/34)\u001b[K\rremote: Counting objects:  52% (18/34)\u001b[K\rremote: Counting objects:  55% (19/34)\u001b[K\rremote: Counting objects:  58% (20/34)\u001b[K\rremote: Counting objects:  61% (21/34)\u001b[K\rremote: Counting objects:  64% (22/34)\u001b[K\rremote: Counting objects:  67% (23/34)\u001b[K\rremote: Counting objects:  70% (24/34)\u001b[K\rremote: Counting objects:  73% (25/34)\u001b[K\rremote: Counting objects:  76% (26/34)\u001b[K\rremote: Counting objects:  79% (27/34)\u001b[K\rremote: Counting objects:  82% (28/34)\u001b[K\rremote: Counting objects:  85% (29/34)\u001b[K\rremote: Counting objects:  88% (30/34)\u001b[K\rremote: Counting objects:  91% (31/34)\u001b[K\rremote: Counting objects:  94% (32/34)\u001b[K\rremote: Counting objects:  97% (33/34)\u001b[K\rremote: Counting objects: 100% (34/34)\u001b[K\rremote: Counting objects: 100% (34/34), done.\u001b[K\n","remote: Compressing objects:   3% (1/27)\u001b[K\rremote: Compressing objects:   7% (2/27)\u001b[K\rremote: Compressing objects:  11% (3/27)\u001b[K\rremote: Compressing objects:  14% (4/27)\u001b[K\rremote: Compressing objects:  18% (5/27)\u001b[K\rremote: Compressing objects:  22% (6/27)\u001b[K\rremote: Compressing objects:  25% (7/27)\u001b[K\rremote: Compressing objects:  29% (8/27)\u001b[K\rremote: Compressing objects:  33% (9/27)\u001b[K\rremote: Compressing objects:  37% (10/27)\u001b[K\rremote: Compressing objects:  40% (11/27)\u001b[K\rremote: Compressing objects:  44% (12/27)\u001b[K\rremote: Compressing objects:  48% (13/27)\u001b[K\rremote: Compressing objects:  51% (14/27)\u001b[K\rremote: Compressing objects:  55% (15/27)\u001b[K\rremote: Compressing objects:  59% (16/27)\u001b[K\rremote: Compressing objects:  62% (17/27)\u001b[K\rremote: Compressing objects:  66% (18/27)\u001b[K\rremote: Compressing objects:  70% (19/27)\u001b[K\rremote: Compressing objects:  74% (20/27)\u001b[K\rremote: Compressing objects:  77% (21/27)\u001b[K\rremote: Compressing objects:  81% (22/27)\u001b[K\rremote: Compressing objects:  85% (23/27)\u001b[K\rremote: Compressing objects:  88% (24/27)\u001b[K\rremote: Compressing objects:  92% (25/27)\u001b[K\rremote: Compressing objects:  96% (26/27)\u001b[K\rremote: Compressing objects: 100% (27/27)\u001b[K\rremote: Compressing objects: 100% (27/27), done.\u001b[K\n","Receiving objects:   2% (1/34)   \rReceiving objects:   5% (2/34)   \rReceiving objects:   8% (3/34)   \rReceiving objects:  11% (4/34)   \rReceiving objects:  14% (5/34)   \rReceiving objects:  17% (6/34)   \rReceiving objects:  20% (7/34)   \rReceiving objects:  23% (8/34)   \rReceiving objects:  26% (9/34)   \rReceiving objects:  29% (10/34)   \rReceiving objects:  32% (11/34)   \rReceiving objects:  35% (12/34)   \rReceiving objects:  38% (13/34)   \rReceiving objects:  41% (14/34)   \rReceiving objects:  44% (15/34)   \rReceiving objects:  47% (16/34)   \rReceiving objects:  50% (17/34)   \rReceiving objects:  52% (18/34)   \rReceiving objects:  55% (19/34)   \rReceiving objects:  58% (20/34)   \rReceiving objects:  61% (21/34)   \rReceiving objects:  64% (22/34)   \rReceiving objects:  67% (23/34)   \rremote: Total 34 (delta 11), reused 16 (delta 4), pack-reused 0\u001b[K\n","Receiving objects:  70% (24/34)   \rReceiving objects:  73% (25/34)   \rReceiving objects:  76% (26/34)   \rReceiving objects:  79% (27/34)   \rReceiving objects:  82% (28/34)   \rReceiving objects:  85% (29/34)   \rReceiving objects:  88% (30/34)   \rReceiving objects:  91% (31/34)   \rReceiving objects:  94% (32/34)   \rReceiving objects:  97% (33/34)   \rReceiving objects: 100% (34/34)   \rReceiving objects: 100% (34/34), 848.20 KiB | 11.46 MiB/s, done.\n","Resolving deltas:   0% (0/11)   \rResolving deltas:  54% (6/11)   \rResolving deltas:  63% (7/11)   \rResolving deltas: 100% (11/11)   \rResolving deltas: 100% (11/11), done.\n","/content/cloned-repo\n","'01-RNN-Bioprocess-on-V1.2.3-9 Column - CPU.ipynb'   reduced_training_data.xlsx\n"," NewDatasets2.xlsx\t\t\t\t     test_data.xlsx\n"," NewDatasets.xlsx\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ICsyiYZRxOT9","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1598000006905,"user_tz":-60,"elapsed":1268185,"user":{"displayName":"Ary Mega","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhprJbEh5igyOrqL87s-U1xxnUAI4rx9VWMM98Llw=s64","userId":"10199364316992018908"}},"outputId":"d506a872-7255-40b6-a478-437ed9a3b768"},"source":["import torch\n","import pandas as pd\n","import numpy as np \n","import os\n","import time\n","start_time = time.time()\n","#from ann2 import Net\n","#from replicate import replicate_data \n","from sklearn.preprocessing import StandardScaler\n","#from train import train\n","#from test2 import test\n","\n","# Load training and testing data as pd dataframe\n","training_data = pd.read_excel('/content/cloned-repo/NewDatasets2.xlsx', sheet_name='IOrtrain (1)')\n","testing_data = pd.read_excel('/content/cloned-repo/NewDatasets2.xlsx', sheet_name='IOrtest (1)')\n","\n","# Standardise training and testing data\n","scaler_train = StandardScaler()\n","scaler_test = StandardScaler()\n","\n","scaler_train.fit(training_data)\n","scaler_test.fit(testing_data)\n","\n","testing_data = scaler_test.transform(testing_data)\n","\n","# Convert training data to pd dataframe\n","columns = \"BC NC LP NIC LI\".split()\n","training_data = pd.DataFrame(data=training_data, index=None, columns=columns)\n","\n","# Replicate the training data\n","replicated_data1 = replicate_data(training_data, 50, 0.03)\n","replicated_data2 = replicate_data(training_data, 50, 0.05)\n","\n","training_data = training_data.append(replicated_data1, ignore_index=True, sort=False)\n","training_data = training_data.append(replicated_data2, ignore_index=True, sort=False)\n","\n","training_data = scaler_train.transform(training_data)\n","training_data = np.array(training_data)\n","\n","# Calculate training and testing labels\n","try:\n","    a = []\n","    for index, row in enumerate(training_data):\n","        dBC = training_data[index + 1][0] - row[0]\n","        dNC = training_data[index + 1][1] - row[1]\n","        dLP = training_data[index + 1][2] - row[2]\n","        dNIC = training_data[index + 1][3] - row[3]\n","\n","        rates = [dBC, dNC, dLP, dNIC]\n","        a.append(rates)\n","except IndexError:\n","    rates = [0, 0, 0, 0]\n","    a.append(rates)\n","\n","a = np.array(a)\n","training_data = np.append(training_data, a, axis=1)\n","\n","try:\n","    a = []\n","    for index, row in enumerate(testing_data):\n","        dBC = testing_data[index + 1][0] - row[0]\n","        dNC = testing_data[index + 1][1] - row[1]\n","        dLP = testing_data[index + 1][2] - row[2]\n","        dNIC = testing_data[index + 1][3] - row[3]\n","\n","        rates = [dBC, dNC, dLP, dNIC]\n","        a.append(rates)\n","except IndexError:\n","    rates = [0, 0, 0, 0]\n","    a.append(rates)\n","\n","a = np.array(a)\n","testing_data = np.append(testing_data, a, axis=1)\n","\n","# Remove all datapoints corresponding to 144 h from the training and testing sets\n","count = 0\n","decrement = 0\n","for index, row in enumerate(training_data):\n","    count += 1\n","    if count == 15:\n","        delete = index - decrement\n","        training_data = np.delete(training_data, delete, 0)\n","        decrement += 1\n","        count = 0\n","\n","count = 0\n","decrement = 0\n","for index, row in enumerate(testing_data):\n","    count += 1\n","    if count == 15:\n","        delete = index - decrement\n","        testing_data = np.delete(testing_data, delete, 0)\n","        decrement += 1\n","        count = 0\n","\n","# Shuffle training data\n","np.random.shuffle(training_data)\n","\n","# Define structure of optimal network\n","HL = 2\n","HN1, HN2 = 14, 10\n","EPOCHS = 476\n","BATCH_SIZE = 100\n","LR = 0.004\n","\n","xcl_dir = '/content/drive/My Drive/Colab Notebooks/GitHub/FNN/ANN/Run 5/Results/2HL/' #create a new folder for prediction rsults\n","try:\n","  os.mkdir(xcl_dir)\n","except:\n","  pass\n","\n","\n","# Instantiate the network and prepare data\n","for count in range(1):\n","  avg_mse=10\n","  min_mse=10\n","\n","  #while avg_mse > 0.006:\n","  while count < 50:\n","    net = Net(HN1, HN2)\n","    training_inputs = training_data[:, 0:5]\n","    training_labels = training_data[:, 5:]\n","    test_inputs = testing_data[:, 0:5]\n","    test_labels = testing_data[:, 5:]\n","\n","    # Train and test the network\n","    train(net, training_inputs, training_labels, EPOCHS, LR, BATCH_SIZE)\n","    avg_mse, predictions_online, predictions_offline = test(test_inputs, test_labels, net)\n","    #print(avg_mse)\n","\n","    count = count + 1\n","    if min_mse >= avg_mse or count==50 : #count=* is related to while count above\n","      min_mse = avg_mse\n","      count_min = count\n","      # Save file every minimum found\n","\n","      predictions_online_inverse_transform = scaler_test.inverse_transform(predictions_online)\n","      predictions_offline_inverse_transform = scaler_test.inverse_transform(predictions_offline)\n","\n","      online = pd.DataFrame(predictions_online_inverse_transform)\n","      offline = pd.DataFrame(predictions_offline_inverse_transform)\n","      avg_mse = pd.DataFrame([avg_mse, 0])\n","      f= round(min_mse.item(), 5)\n","\n","      with pd.ExcelWriter('{xcl_dir}Predictions V1.1 run5 {count}_{f}_{x}_{y}-{z}_{a}_{b}_{c}.xlsx'.format(xcl_dir=xcl_dir, x=HL, y=HN1, z=HN2, a=EPOCHS, b=LR, c=BATCH_SIZE, count=count, f=f)) as writer:  \n","          offline.to_excel(writer, sheet_name='Offline', startrow=1, startcol=1)\n","          online.to_excel(writer, sheet_name='Online', startrow=1, startcol=1)\n","          avg_mse.to_excel(writer, sheet_name='Avg_MSE', startrow=1, startcol=1)\n","      torch.save(net.state_dict(), '{xcl_dir}Model V1.1 run5 {count}_{f}_{x}_{y}-{z}_{a}_{b}_{c}.pt'.format(xcl_dir=xcl_dir, x=HL, y=HN1, z=HN2, a=EPOCHS, b=LR, c=BATCH_SIZE, count=count, f=f))\n","    print(avg_mse, min_mse, count, count_min)\n","\n","print(f'\\nDuration: {time.time() - start_time:.0f} seconds')\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["         0\n","0  0.10844\n","1  0.00000 0.10844043219045377 1 1\n","0.3030166205635789 0.10844043219045377 2 1\n","0.16143285957938586 0.10844043219045377 3 1\n","0.17669881700112794 0.10844043219045377 4 1\n","0.16345333985848265 0.10844043219045377 5 1\n","0.28901129833145256 0.10844043219045377 6 1\n","0.1714227305881409 0.10844043219045377 7 1\n","0.13485995216589433 0.10844043219045377 8 1\n","0.23119452748747157 0.10844043219045377 9 1\n","0.15284249824916255 0.10844043219045377 10 1\n","0.15967231996704115 0.10844043219045377 11 1\n","0.11369334928563041 0.10844043219045377 12 1\n","0.1861806620191431 0.10844043219045377 13 1\n","0.14012609846675717 0.10844043219045377 14 1\n","0.24239881768371008 0.10844043219045377 15 1\n","0.13265184321452828 0.10844043219045377 16 1\n","0.14638967787471113 0.10844043219045377 17 1\n","0.3695253785366585 0.10844043219045377 18 1\n","0.14336760127758597 0.10844043219045377 19 1\n","0.21740594721505777 0.10844043219045377 20 1\n","0.11260607675460937 0.10844043219045377 21 1\n","0.196631071557913 0.10844043219045377 22 1\n","0.15791864127645727 0.10844043219045377 23 1\n","0.13023276419428384 0.10844043219045377 24 1\n","0.19438977043447375 0.10844043219045377 25 1\n","0.18223460074844694 0.10844043219045377 26 1\n","0.12813915570474751 0.10844043219045377 27 1\n","0.29321889930577827 0.10844043219045377 28 1\n","0.4483811519864228 0.10844043219045377 29 1\n","0.15370708770415842 0.10844043219045377 30 1\n","0.34432033873839824 0.10844043219045377 31 1\n","0.2523985516560744 0.10844043219045377 32 1\n","          0\n","0  0.096607\n","1  0.000000 0.09660733157286097 33 33\n","0.12559518672649272 0.09660733157286097 34 33\n","0.1418099255427139 0.09660733157286097 35 33\n","0.21108884105334855 0.09660733157286097 36 33\n","0.35352607432463284 0.09660733157286097 37 33\n","0.16457943579339315 0.09660733157286097 38 33\n","          0\n","0  0.091614\n","1  0.000000 0.09161384259311543 39 39\n","0.21978410272526058 0.09161384259311543 40 39\n","0.20017032469295273 0.09161384259311543 41 39\n","0.1564805441019123 0.09161384259311543 42 39\n","0.12053395389467886 0.09161384259311543 43 39\n","0.17231693357371428 0.09161384259311543 44 39\n","0.13973357487186608 0.09161384259311543 45 39\n","0.14155800911359323 0.09161384259311543 46 39\n","0.32494848618175715 0.09161384259311543 47 39\n","0.1941206070553943 0.09161384259311543 48 39\n","0.14831829150558606 0.09161384259311543 49 39\n","          0\n","0  0.560619\n","1  0.000000 0.560619392073989 50 50\n","\n","Duration: 1259 seconds\n"],"name":"stdout"}]}]}