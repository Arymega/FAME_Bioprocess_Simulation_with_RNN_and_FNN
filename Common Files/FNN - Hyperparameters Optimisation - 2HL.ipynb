{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"FNN - Hyperparameters Optimisation - 2HL.ipynb","provenance":[{"file_id":"1sqdWsP9aVxs2QHJX30lr1z6E-QfWf7qq","timestamp":1595259072451},{"file_id":"1MRoEq2_1zDfqfhBZ9OorrtImxrIXP9ss","timestamp":1592721766639},{"file_id":"1Zq17yRxUskck6WrqCtDBs2sT3H8cASzz","timestamp":1591904362600},{"file_id":"1GV8hcETq8HS8vPmOaC4Z7fJcR2Hterma","timestamp":1591263473503}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"yMzLsNEuuMNu","colab_type":"text"},"source":["# Ray Tune Environment Starter"]},{"cell_type":"code","metadata":{"id":"fxG7EF0BAUFh","colab_type":"code","cellView":"both","colab":{}},"source":["## Dependencies for Google Colab environment \n","\n","print(\"Setting up colab environment\")\n","!pip uninstall -y -q pyarrow\n","!pip install -q https://s3-us-west-2.amazonaws.com/ray-wheels/latest/ray-0.8.0.dev5-cp36-cp36m-manylinux1_x86_64.whl\n","!pip install -q ray[debug]\n","\n","# # A hack to force the runtime to restart, needed to include the above dependencies.\n","print(\"Done installing! Restarting via forced crash (this is not an issue).\")\n","import os\n","os._exit(0)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KbEmnzkzAuNd","colab_type":"text"},"source":["## TensorFLow for Colab"]},{"cell_type":"code","metadata":{"id":"69jc5XEFZ7a4","colab_type":"code","colab":{}},"source":["## TensorFlow 2.0 for Colab\n","\n","try:\n","   # %tensorflow_version only exists in Colab.\n","  %tensorflow_version 2.x\n","except Exception:\n","  pass"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RJr5YOgRA-8D","colab_type":"text"},"source":["## Ray Tune Library Import"]},{"cell_type":"code","metadata":{"id":"dBcrMKeo0FzU","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","try:\n","    tf.get_logger().setLevel('INFO')\n","except Exception as exc:\n","    print(exc)\n","import warnings\n","warnings.simplefilter(\"ignore\")\n","\n","import os\n","import numpy as np\n","import torch\n","import torch.optim as optim\n","from torchvision import datasets\n","from ray.tune.examples.mnist_pytorch import get_data_loaders, ConvNet \n","\n","import ray\n","from ray import tune\n","from ray.tune import track\n","from ray.tune.schedulers import PopulationBasedTraining\n","from ray.tune.util import validate_save_restore\n","from ray.tune.schedulers import AsyncHyperBandScheduler\n","\n","%matplotlib inline\n","import matplotlib.style as style\n","import matplotlib.pyplot as plt\n","style.use(\"ggplot\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eS6S_2KFjOCu","colab_type":"text"},"source":["# Function codes"]},{"cell_type":"markdown","metadata":{"id":"9qr7um8qjLR2","colab_type":"text"},"source":["## Test Function"]},{"cell_type":"code","metadata":{"id":"7DiNXvz_jJRM","colab_type":"code","colab":{}},"source":["import torch\n","import numpy as np\n","import pandas as pd\n","\n","\n","def test(test_inputs, test_labels, net):\n","    test_X = torch.Tensor(test_inputs).view(-1, 5)\n","    test_y = torch.Tensor(test_labels)\n","\n","    predictionNumpy = []\n","    with torch.no_grad():\n","        for i in range(0, len(test_X)):\n","            net_out = net(test_X[i].view(-1, 5))\n","            predictionNumpy.append(net_out[0].numpy())              # The output from the net is a tensor which contains only one element which is a list. The list contains the 3 output values. We only want the list, not the tensoor containing one element which is a list.\n","\n","    experimental = []\n","    for data in test_y:\n","        experimental.append(data.numpy())\n","\n","    squared_error_X = []\n","    squared_error_N = []\n","    squared_error_L = [] \n","    squared_error_C = [] \n","\n","    for i in range(0, len(experimental)):\n","            X_error = experimental[i][0] - predictionNumpy[i][0]\n","            N_error = experimental[i][1] - predictionNumpy[i][1]\n","            L_error = experimental[i][2] - predictionNumpy[i][2]\n","            C_error = experimental[i][3] - predictionNumpy[i][3]\n","            squared_error_X.append(X_error**2)\n","            squared_error_N.append(N_error**2)\n","            squared_error_L.append(L_error**2)\n","            squared_error_C.append(C_error**2)\n","\n","    MSE_X1 = sum(squared_error_X[0:14])/14\n","    MSE_N1 = sum(squared_error_N[0:14])/14\n","    MSE_L1 = sum(squared_error_L[0:14])/14\n","    MSE_C1 = sum(squared_error_L[0:14])/14\n","    \n","    MSE_X2 = sum(squared_error_X[14:28])/14         # To accomodate more than 1 test sets \n","    MSE_N2 = sum(squared_error_N[14:28])/14\n","    MSE_L2 = sum(squared_error_L[14:28])/14\n","    MSE_C2 = sum(squared_error_C[14:28])/14\n","    MSE_list = [MSE_X1, MSE_N1, MSE_L1, MSE_C1, MSE_X2, MSE_N2, MSE_L2, MSE_C2]\n","    AVG_MSE = sum(MSE_list)/8\n","\n","    return AVG_MSE\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R6ZNpdCtjHI1","colab_type":"text"},"source":["## Train Function"]},{"cell_type":"code","metadata":{"id":"upghiKsCjFRK","colab_type":"code","colab":{}},"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from tqdm import tqdm\n","\t\t\t\t\t  \n","def train(net, inputs, labels, EPOCHS, l_rate, BATCH_SIZE):\n","\tnet.train()                                                                         \n","\toptimiser = optim.Adam(net.parameters(), lr = l_rate)\t\t\t\t\t\t\t\t\t   # net.parameters(): all of the adjustable parameters in our network. lr: a hyperparameter adjusts the size of the step that the optimizer will take to minimise the loss.\n","\tloss_function = nn.MSELoss(reduction='mean')\n","\n","\tX = torch.Tensor(inputs).view(-1, 5)\n","\ty = torch.Tensor(labels)\n","\t\t\t \t\t   \n","\tfor epoch in range(EPOCHS):\n","\t\tfor i in tqdm(range(0, len(X), BATCH_SIZE)):\n","\t\t\tbatch_X = X[i:i+BATCH_SIZE].view(-1, 5)\n","\t\t\tbatch_y = y[i:i+BATCH_SIZE]\n","\n","\t\t\toptimiser.zero_grad()\n","\t\t\toutputs = net(batch_X)\n","\t\t\tloss = loss_function(outputs, batch_y)\n","\t\t\tloss.backward()\n","\t\t\toptimiser.step()\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Zt7wZlZ2jCvP","colab_type":"text"},"source":["## Replication Function"]},{"cell_type":"code","metadata":{"id":"I2pRV5pEi-F_","colab_type":"code","colab":{}},"source":["import numpy as np\n","import pandas as pd\n","\n","## Create a function which takes in a dataset and replicates it\n","def replicate_data(data, replications, noise):                                                      # Create a function which accepts a dataset and replicates it\n","    cols = list(data.columns) \n","    dataR = data[cols[0:4]]                                                                         # Create a secondary dataframe containing only columns 1-3 (the columns we want to replicate)\n","    df = data                                                                                       # Create the output dataframe that will contain both the original and the replicated data\n","    new_data = pd.DataFrame(columns=data.columns)\n","    i = 0                                                                                           # Initialise replication counter to 0\n","    while i < replications:\n","        replicated_data =  np.random.uniform(dataR-dataR*noise, dataR+dataR*noise)                  # Create random noise for each value in columns 2-4 of dataset\n","        replicated_data = pd.DataFrame(data=replicated_data, index=None, columns=dataR.columns)      # Cast the replicated data as a pandas DataFrame Object\n","        replicated_data['LI'] = df[cols[4]]                                                          # Add the missing light intensity column back into the replicated_data set\n","        new_data = new_data.append(replicated_data, ignore_index=True, sort=False)\n","        i += 1\n","    return new_data\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oDrCOY4Fiqcz","colab_type":"text"},"source":["## FNN Model"]},{"cell_type":"code","metadata":{"id":"WkcLPynaikIW","colab_type":"code","colab":{}},"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","torch.manual_seed(777)\n","\n","class Net(nn.Module):\n","\t'''\n","\tThis Class Defines the Structure of the Artificial Neural Network\n","\t'''\n","\tdef __init__(self, HN1, HN2):\t\n","\t\tself.HN1 = HN1\t\t\t\n","\t\tself.HN2 = HN2\n","\t\tsuper().__init__()                            # Run the intitialision method from base class nn.module.\n","\t\tself.fc1 = nn.Linear(5, self.HN1)             # Define the first fully connected layer. nn.Linear simply connects the input nodes to the output nodes in the standard way. The input layer contains 5 nodes.                                                            The output layer (first hidden layer), consists of 15 nodes.\n","\t\tself.fc2 = nn.Linear(self.HN1, self.HN2)      # Hidden layer 2: each node takes in 15 values, contains 15 nodes hence outputs 15 values.\n","\t\tself.fc3 = nn.Linear(self.HN2, 4)             # Output Layer: each node takes in 15 values, contain 4 nodes (one for each rate of change: X, N and Lu) hence outputs 4 (ary: previously 3) values.\n","\n","\tdef forward(self, x):                             # This method feeds data into the network and propagates it forward.\n","\t\tx = torch.sigmoid(self.fc1(x))                # Feed the dataset, x, through fc1 and apply the Sigmoid activation function to the weighted sum of each neuron. Then assign the transformed dataset to x. Next,                                                        feed the transformed dataset through fc2 and so on... until we reach the output layer. The activation fucntion basically decides if the neuron is 'firing'                                                                like real neurones in the human brain. The activation function prevents massive output numbers.\n","\t\tx = torch.sigmoid(self.fc2(x))\n","\t\tx = self.fc3(x)\n","\t\treturn x \n"," \n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3v8NUw-aizVN","colab_type":"text"},"source":["## Data Sets Import"]},{"cell_type":"code","metadata":{"id":"i9iPrwuk5KJ8","colab_type":"code","colab":{}},"source":["# Clone the entire repo.\n","!git clone -l -s git://github.com/Arymega/FAME_Bioprocess_Simulation_with_RNN_and_FNN.git cloned-repo\n","%cd cloned-repo\n","!ls"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-JjeRZXxEij2","colab_type":"text"},"source":["## FNN Configuration"]},{"cell_type":"code","metadata":{"id":"_exsrm8FYZkQ","colab_type":"code","colab":{}},"source":["import pandas as pd\n","import numpy as np \n","import copy\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import KFold\n","import csv\n","import time\n","start_time = time.time()\n","\n","# Load training data as pd dataframe and convert pd dataframe into numpy array.\n","training_data = pd.read_excel('/content/cloned-repo/Common Files/Datasets.xlsx', sheet_name='6Sets')\n","training_data_array = np.array(training_data)\n","\n","# Standardise Training Data\n","scaler_train = StandardScaler()\n","scaler_train.fit(training_data)\n","\n","# Split data into k=6 folds.\n","kf = KFold(n_splits=6)\n","kf.get_n_splits(training_data)\n","\n","# Split training data set into 6 subsets containing k-1 folds before optimisation.\n","class wrapper(object):\n","    def __init__(self):\n","        self.value = []\n","\n","subset_train1 = wrapper() \n","subset_train2 = wrapper()\n","subset_train3 = wrapper()\n","subset_train4 = wrapper()\n","subset_train5 = wrapper()\n","subset_train6 = wrapper()\n","subset_test1 = wrapper() \n","subset_test2 = wrapper()\n","subset_test3 = wrapper()\n","subset_test4 = wrapper()\n","subset_test5 = wrapper()\n","subset_test6 = wrapper()\n","subset_train_list = [subset_train1, subset_train2, subset_train3, subset_train4, subset_train5, subset_train6]\n","subset_test_list = [subset_test1, subset_test2, subset_test3, subset_test4, subset_test5, subset_test6]\n","\n","index = 0\n","for train_index, test_index in kf.split(training_data):\n","\n","    for row in train_index:\n","        subset_train_list[index].value.append(training_data_array[row])\n","    \n","    for row in test_index:\n","        subset_test_list[index].value.append(training_data_array[row])\n","    \n","    index +=1\n","\n","\n","# Standardise Test Data\n","for subset in subset_test_list:\n","    subset.value = scaler_train.transform(subset.value)\n","\n","# Replicate and Standardise the training data in each subset.\n","\n","columns = \"B N F NIC LI\".split()\n","for index, subset in enumerate(subset_train_list):\n","    df = pd.DataFrame(data=subset.value, index=None, columns=columns)\n","    ref = df\n","    df = scaler_train.transform(df)\n","\n","    replicated_data1 = replicate_data(ref, 50, 0.03)\n","    replicated_data1 = scaler_train.transform(replicated_data1)\n","    df = np.append(df, replicated_data1, axis=0) \n","\n","    replicated_data2 = replicate_data(ref, 50, 0.05)\n","    replicated_data2 = scaler_train.transform(replicated_data2)\n","    df = np.append(df, replicated_data2, axis=0) \n","\n","    subset.value = df\n","\n","\n","# Calculate training and test labels\n","for index1, subset in enumerate(subset_train_list):\n","    a = []\n","    \n","    try:\n","        for index2, row in enumerate(subset.value):\n","            dB = subset.value[index2 + 1][0] - row[0]\n","            dN = subset.value[index2 + 1][1] - row[1]\n","            dF = subset.value[index2 + 1][2] - row[2]\n","            dNIC = subset.value[index2 + 1][3] - row[3]\n","\n","            rates =[dB, dN, dF, dNIC]\n","            a.append(rates)\n","    except IndexError:\n","        rates = [0, 0, 0, 0]\n","        a.append(rates)\n","    \n","    a = np.array(a)\n","    subset.value = np.append(subset.value, a, axis=1) \n","\n","for index1, subset in enumerate(subset_test_list):\n","    b = []\n","    \n","    try:\n","        for index2, row in enumerate(subset.value):\n","            dB = subset.value[index2 + 1][0] - row[0] \n","            dN = subset.value[index2 + 1][1] - row[1]\n","            dF = subset.value[index2 + 1][2] - row[2]\n","            dNIC = subset.value[index2 + 1][3] - row[3]\n","\n","            rates =[dB, dN, dF, dNIC]\n","            b.append(rates)\n","    except IndexError:\n","        rates = [0, 0, 0, 0]\n","        b.append(rates)\n","    \n","    b = np.array(b)\n","    subset.value = np.append(subset.value, b, axis=1)\n","\n","\n","# Remove all 15th datapoints from corresponding training and testing sets\n","for subset in subset_train_list:\n","    count = 0\n","    decrement = 0\n","    for index, row in enumerate(subset.value):\n","        count +=1\n","        if count == 15:\n","            delete = index - decrement\n","            subset.value = np.delete(subset.value, delete, 0)\n","            decrement += 1\n","            count = 0\n","\n","for subset in subset_test_list:\n","    subset.value = np.delete(subset.value, -1, 0)\n","\n","subset_train_list = np.array(subset_train_list)\n","subset_test_list = np.array(subset_test_list)\n","\n","# Shuffle Training Data\n","for subset in subset_train_list:\n","    np.random.shuffle(subset.value)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YNEWIsxcd3xx","colab_type":"text"},"source":["## Train_tune definition\n"]},{"cell_type":"code","metadata":{"id":"xh0o8W1Xd298","colab_type":"code","colab":{}},"source":["# k-fold cross validation training loop\n","HL = 2   #Initial Configurations\n","HN1 = 5 \n","HN2 = 6 \n","EPOCHS = 3 \n","BATCH_SIZE = 100\n","LR = 0.001\n","MODELS = {}\n","h1 = HN1\n","h2 = HN2\n","e = EPOCHS\n","momentum = 0.9\n","\n","def train_tune(config):\n","  lr=config[\"lr\"]\n","  h1 = config[\"h1\"]\n","  h2 = config[\"h2\"]\n","  e=config[\"e\"]\n","  net = Net(h1, h2)\n","  init_state = copy.deepcopy(net.state_dict())\n"," \n","  MSEs = []\n","\n","  for count in range(iteration):\n","\n","    for index, subset in enumerate(subset_train_list):\n","\n","      subset.value = np.array(subset.value)\n","      subset_test_list[index].value = np.array(subset_test_list[index].value)\n","      \n","      net.load_state_dict(init_state)\n","\n","      training_inputs = subset.value[:, 0:5]\n","      training_labels = subset.value[:, 5:]\n","      test_inputs = subset_test_list[index].value[:, 0:5]\n","      test_labels = subset_test_list[index].value[:, 5:]\n","\n","      train(net, training_inputs, training_labels, e, LR, BATCH_SIZE)\n","\n","      avg_mse = test(test_inputs, test_labels, net)\n","      MSEs.append(avg_mse)\n","\n","    acc = sum(MSEs)/len(MSEs)\n","    tune.track.log(mean_accuracy=acc.item())\n","    MODELS['F{a}-{b}_{x}-{y}_{z}'.format(a=index+1, b=HL, x=h1, y=h2, z=e)] = acc\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6PcIfT0o4IYm","colab_type":"text"},"source":["## Ray Tune Configuration 2"]},{"cell_type":"code","metadata":{"id":"824Fkq8hxgBt","colab_type":"code","colab":{}},"source":["class PytorchTrainble(tune.Trainable):\n","    def _setup(self, config):\n","        self.device = torch.device(\"cpu\")\n","        self.train_loader, self.test_loader = get_data_loaders()\n","        self.model = ConvNet().to(self.device)\n","        self.optimizer = optim.SGD(\n","            self.model.parameters(),\n","            lr=config.get(\"lr\", 0.01),\n","            momentum=config.get(\"momentum\", 0.9))\n","\n","    def _train(self):\n","        net = Net(h1, h2)\n","        init_state = copy.deepcopy(net.state_dict())\n","        \n","        MSEs = []\n","        iteration = 1\n","        for count in range(iteration):\n","\n","            for index, subset in enumerate(subset_train_list):\n","\n","                subset.value = np.array(subset.value)\n","                subset_test_list[index].value = np.array(subset_test_list[index].value)\n","                \n","                net.load_state_dict(init_state)\n","\n","                training_inputs = subset.value[:, 0:5]\n","                training_labels = subset.value[:, 5:]\n","                test_inputs = subset_test_list[index].value[:, 0:5]\n","                test_labels = subset_test_list[index].value[:, 5:]\n","\n","                train(net, training_inputs, training_labels, e, LR, BATCH_SIZE)\n","\n","                avg_mse = test(test_inputs, test_labels, net)\n","                MSEs.append(avg_mse)\n","\n","            acc = sum(MSEs)/len(MSEs)\n","    \n","            return {\"mean_accuracy\": acc.item()}            \n","\n","    def _save(self, checkpoint_dir):\n","        checkpoint_path = os.path.join(checkpoint_dir, \"model.pth\")\n","        torch.save(self.model.state_dict(), checkpoint_path)\n","        return checkpoint_path\n","\n","    def _restore(self, checkpoint_path):\n","        self.model.load_state_dict(torch.load(checkpoint_path))\n","        \n","    def reset_config(self, new_config):\n","        del self.optimizer\n","        self.optimizer = optim.SGD(\n","            self.model.parameters(),\n","            lr=new_config.get(\"lr\", 0.01),\n","            momentum=new_config.get(\"momentum\", 0.9))\n","        return True\n","\n","\n","ray.shutdown()  # Restart Ray defensively in case the ray connection is lost. \n","ray.init(log_to_driver=False)\n","\n","validate_save_restore(PytorchTrainble)\n","validate_save_restore(PytorchTrainble, use_object_store=True)\n","print(\"Success!\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2Hr-6Dz9GEca","colab_type":"text"},"source":["##PBT Configuration"]},{"cell_type":"code","metadata":{"id":"3oShba7qzNms","colab_type":"code","colab":{}},"source":["scheduler = PopulationBasedTraining(\n","    time_attr=\"training_iteration\",\n","    metric=\"mean_accuracy\",\n","    mode=\"min\",\n","    perturbation_interval=5,\n","    hyperparam_mutations={\n","        # distribution for resampling\n","        \"lr\": lambda: np.random.uniform(0.0001, 0.0005),\n","        # allow perturbations within this set of categorical values\n","        \"momentum\": [0.8, 0.9, 0.99],\n","    }\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"31TMPSxi2aX2","colab_type":"code","colab":{}},"source":["ray.shutdown()  # Restart Ray defensively in case the ray connection is lost. \n","ray.init(log_to_driver=False)\n","\n","import time\n","start_time = time.time()\n","\n","analysis = tune.run(\n","    PytorchTrainble,\n","    name=\"pbt_test\",\n","    scheduler=scheduler,\n","    reuse_actors=True,\n","    verbose=1,\n","    stop={\n","        \"training_iteration\": 4,\n","    },\n","    num_samples=4,\n","    \n","    # PBT starts by training many neural networks in parallel with random hyperparameters. \n","    config={\n","        \"lr\": tune.choice([0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008, 0.0009, 0.001, 0.002, 0.003, 0.004, 0.005,]),\n","        \"h1\" : tune.randint(5, 20),\n","        \"h2\" : tune.randint(5, 20),\n","        \"e\" : tune.randint(15, 500),\n","        \"momentum\": tune.choice([0.9]),\n","    })\n","\n","print(f'\\nDuration: {time.time() - start_time:.0f} seconds')\n","\n","print('best config:', analysis.get_best_config(\"mean_accuracy\"))\n","\n","# Visualize all mutations of Population-based Training.\n","! cat ~/ray_results/pbt_test/pbt_global.txt\n","\n","# Plot by wall-clock time\n","\n","dfs = analysis.fetch_trial_dataframes()\n","# This plots everything on the same plot\n","ax = None\n","for d in dfs.values():\n","    ax = d.plot(\"training_iteration\", \"mean_accuracy\", figsize=(25,5),ax=ax, legend=False)\n","plt.xlabel(\"epoch\"); plt.ylabel(\"Test Accuracy\"); \n","\n","sav_dir = '/content/drive/My Drive/Colab Notebooks/GitHub/MSc/FNN/Results/2HL/' #create a new folder for optimisation results\n","zfile = 'ray_results_2HL.zip'       #Save optimisation results as zip file\n","try:\n","  os.mkdir(sav_dir)\n","except:\n","  pass\n","!zip -r '{sav_dir}{zfile}' /root/ray_results\n","\n","%load_ext tensorboard\n","%tensorboard --logdir ~/ray_results/pbt_test"],"execution_count":null,"outputs":[]}]}