{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"RNN - Model_Train_Test - 1HL.ipynb","provenance":[{"file_id":"1wmdLO3cIEGrmEov2bCXnXDU0dqoLiOEB","timestamp":1594068971720},{"file_id":"1SBemwFy0SqDGYBS-qzZJ-4In-ezci0Gx","timestamp":1591174166298},{"file_id":"16uuXtfy_HZdrZ6JujGXBjfCk0Crlbroa","timestamp":1591169471929}],"collapsed_sections":[],"toc_visible":true,"mount_file_id":"1NuM4ByJD27qnS6wvPE_JoOdF44Yxpk-s","authorship_tag":"ABX9TyNlm43wE7dhJGUE+Nx9GF0u"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"HhJCVx7aQfwW","colab_type":"text"},"source":["## Data Replication"]},{"cell_type":"code","metadata":{"id":"V2DvAYi4QI3W","colab_type":"code","colab":{}},"source":["import numpy as np\n","import pandas as pd\n","\n","## Create a function which takes in a dataset and replicates it\n","def replicate_data(data, replications, noise):                                                      # Create a function which accepts a dataset and replicates it\n","    cols = list(data.columns) \n","    dataR = data[cols[0:4]]                                                                         # Create a secondary dataframe containing only columns 1-3 (the columns we want to replicate)\n","    df = data                                                                                       # Create the output dataframe that will contain both the original and the replicated data\n","    new_data = pd.DataFrame(columns=data.columns)\n","    i = 0                                                                                           # Initialise replication counter to 0\n","    while i < replications:\n","        replicated_data =  np.random.uniform(dataR-dataR*noise, dataR+dataR*noise)                  # Create random noise for each value in columns 2-4 of dataset\n","        replicated_data = pd.DataFrame(data=replicated_data, index=None, columns=dataR.columns)      # Cast the replicated data as a pandas DataFrame Object\n","        replicated_data['LI'] = df[cols[4]]                                                          # Add the missing light intensity column back into the replicated_data set\n","        new_data = new_data.append(replicated_data, ignore_index=True, sort=False)\n","        i += 1\n","    return new_data\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y6qN43JgQqzp","colab_type":"text"},"source":["## Test FunctioN"]},{"cell_type":"code","metadata":{"id":"sD8OXvitQQ33","colab_type":"code","colab":{}},"source":["import torch\n","import numpy as np\n","import pandas as pd\n","from torch.autograd import Variable\n","\n","def test(test_inputs, test_labels, net, BATCH_SIZE):\n","    net.eval()\n","    test_X = Variable(torch.Tensor(test_inputs)) \n","    test_y = Variable(torch.Tensor(test_labels))\n","\n","    hidden = net.init_hidden(test_X)\n","    with torch.no_grad():\n","        net_out, _ = net(test_X, hidden)        # Hidden state not required for manual feeding\n","\n","    squared_error_X = []\n","    squared_error_N = []\n","    squared_error_L = [] \n","    squared_error_C = [] \n","\n","    for index1, element in enumerate(test_y):\n","        for index2, row in enumerate(element):\n","            X_error = row[0] - net_out[index1][index2][0]\n","            N_error = row[1] - net_out[index1][index2][1]\n","            L_error = row[2] - net_out[index1][index2][2]\n","            C_error = row[3] - net_out[index1][index2][3]\n","            squared_error_X.append(X_error**2)\n","            squared_error_N.append(N_error**2)\n","            squared_error_L.append(L_error**2)\n","            squared_error_C.append(C_error**2)\n","\n","\n","    MSE_X1 = sum(squared_error_X[0:14])/14\n","    MSE_N1 = sum(squared_error_N[0:14])/14\n","    MSE_L1 = sum(squared_error_L[0:14])/14\n","    MSE_C1 = sum(squared_error_C[0:14])/14\n","    MSE_X2 = sum(squared_error_X[14:28])/14                        #This code has been designed to be compatible with more than one test data set\n","    MSE_N2 = sum(squared_error_N[14:28])/14\n","    MSE_L2 = sum(squared_error_L[14:28])/14\n","    MSE_C2 = sum(squared_error_C[14:28])/14\n","    MSE_list = [MSE_X1, MSE_N1, MSE_L1, MSE_C1, MSE_X2, MSE_N2, MSE_L2, MSE_C2]\n","    AVG_MSE = sum(MSE_list)/8\n","\n","    LI1, LI2 = test_X[0][0][4], test_X[1][0][4]\n","    predictions_online = []\n","    for index1, element in enumerate(test_X):\n","        if index1 == 0:\n","            for index2, row in enumerate(element):\n","                B = row[0] + net_out[index1][index2][0]\n","                N = row[1] + net_out[index1][index2][1]\n","                F = row[2] + net_out[index1][index2][2]\n","                NIC = row[3] + net_out[index1][index2][3]\n","\n","                predictions_online.append([B, N, F, NIC, LI1])\n","        \n","        if index1 == 1:\n","            for index2, row in enumerate(element):\n","                B = row[0] + net_out[index1][index2][0]\n","                N = row[1] + net_out[index1][index2][1]\n","                F = row[2] + net_out[index1][index2][2]\n","                NIC = row[3] + net_out[index1][index2][3]\n","\n","                predictions_online.append([B, N, F, NIC, LI2])\n","    predictions_online = np.array(predictions_online)\n","\n","    predictions_offline = []\n","    B1, B2 = test_X[0][0][0], test_X[1][0][0]\n","    N1, N2 = test_X[0][0][1], test_X[1][0][1]\n","    F1, F2 = test_X[0][0][2], test_X[1][0][2]\n","    NIC1, NIC2 = test_X[0][0][3], test_X[1][0][3]\n","    net.sequence_length = 1                                                     # We will now be feeding 1 input at a time to the network(offline prediction), start extracting and feeding hidden state per item in a sequence. \n","    for index1, element in enumerate(test_X):\n","        hidden = net.init_hidden(Variable(torch.Tensor([[[]]])))                 # Initialise hidden state with a batch size of 1\n","\n","        for index2, row in enumerate(element):\n","\n","            if index1 == 0:\n","                                                                                 # Feed inputs with a batch size of 1, sequence length of 1 and feature vector length of 5 to the network\n","                net_out, hidden = net(Variable(torch.Tensor([    \n","                    [[B1, N1, F1, NIC1, LI1]]\n","                ])), hidden)\n","                B = B1 + net_out[0][0][0]\n","                N = N1 + net_out[0][0][1]\n","                F = F1 + net_out[0][0][2]\n","                NIC = NIC1 + net_out[0][0][3]\n","                predictions_offline.append([float(B), float(N), float(F), float(NIC), float(LI1)])\n","                B1 = B\n","                N1 = N\n","                F1 = F\n","                NIC1 = NIC\n","            \n","            if index1 == 1:\n","                net_out, hidden = net(Variable(torch.Tensor([    \n","                    [[B2, N2, F2, NIC2, LI2]]\n","                ])), hidden)\n","                B = B2 + net_out[0][0][0]\n","                N = N2 + net_out[0][0][1]\n","                F = F2 + net_out[0][0][2]\n","                NIC = NIC2 + net_out[0][0][3]\n","                predictions_offline.append([float(B), float(N), float(F), float(NIC), float(LI2)])\n","                B2 = B\n","                N2 = N\n","                F2 = F\n","                NIC2 = NIC\n","    predictions_offline = np.array(predictions_offline)\n","\n","    return AVG_MSE, predictions_online, predictions_offline\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NwhUaDc5QRAH","colab_type":"code","colab":{}},"source":["#from train import train\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from tqdm import tqdm\n","from torch.autograd import Variable\n","\n","def train(net, inputs, labels, EPOCHS, l_rate, BATCH_SIZE):\n","    net.train()                                                                         \n","    optimiser = optim.Adam(net.parameters(), lr = l_rate)\t\t\t\t\t\t\t\t\t   # net.parameters(): all of the adjustable parameters in our network. lr: a hyperparameter adjusts the size of the step that the optimizer will take to minimise the loss.\n","    loss_function = nn.MSELoss(reduction='mean')\n","\n","    X = Variable(torch.Tensor(inputs))\n","    y = Variable(torch.Tensor(labels))\n","\n","    for epoch in range(EPOCHS):\n","        for i in tqdm(range(0, len(X), BATCH_SIZE), disable=True):\n","            batch_X = X[i:i+BATCH_SIZE]\n","            batch_y = y[i:i+BATCH_SIZE]\n","            hidden = net.init_hidden(batch_X)\n","            optimiser.zero_grad()\n","            outputs, _ = net(batch_X, hidden)\n","            loss = loss_function(outputs, batch_y)\n","            loss.backward()\n","            optimiser.step()\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UBe0PlulUJVd","colab_type":"text"},"source":["##RNN Structure"]},{"cell_type":"code","metadata":{"id":"URTF75Q6dnfX","colab_type":"code","colab":{}},"source":["import torch\n","import torch.nn as nn\n","from torch.autograd import Variable\n","\n","torch.manual_seed(777)\n","\n","### Model ###\n","class RNN(nn.Module):\n","    def __init__(self, num_outputs, input_size, sequence_length, hidden_size, num_layers):\n","        super(RNN, self).__init__()\n","\n","        self.num_outputs = num_outputs\n","        self.input_size = input_size\n","        self.sequence_length = sequence_length\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","\n","        self.rnn = nn.RNN(input_size=self.input_size, hidden_size=self.hidden_size, num_layers=self.num_layers, batch_first=True)\n","        self.fc = nn.Linear(self.hidden_size, self.num_outputs)\n","    \n","    def forward(self, x, hidden):\n","        # Reshape input to (batch_size, sequence_length, input_size)\n","        x = x.view(x.size(0), self.sequence_length, self.input_size)\n","\n","        # Propagate input through RNN\n","        # Input: (batch, seq_len, input_size)\n","        out, _ = self.rnn(x, hidden)\n","        fc_out = self.fc(out)\n","        return fc_out, _\n","    \n","    def init_hidden(self, x):\n","        # Initialse hidden and cell states\n","        # (num_layers * num_directions, batch_size, hidden_size)\n","        return Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size))\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QWs6RSxMGfJT","colab_type":"text"},"source":["## Data Sets Import"]},{"cell_type":"code","metadata":{"id":"swHc9YP8vx52","colab_type":"code","colab":{}},"source":["# Clone the entire repo.\n","!git clone -l -s git://github.com/Arymega/FAME_Bioprocess_Simulation_with_RNN_and_FNN.git cloned-repo\n","%cd cloned-repo\n","!ls"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BxWPXEZOUqCU","colab_type":"text"},"source":["## Perform Training"]},{"cell_type":"code","metadata":{"id":"39EqD72WK45W","colab_type":"code","colab":{}},"source":["import time\n","start_time = time.time()\n","import torch\n","import pandas as pd\n","import numpy as np \n","import os\n","from sklearn.preprocessing import StandardScaler\n","\n","# Load training and testing data as pd dataframe\n","training_data = pd.read_excel('/content/cloned-repo/Common Files/Datasets.xlsx', sheet_name='Train')\n","testing_data = pd.read_excel('/content/cloned-repo/Common Files/Datasets.xlsx', sheet_name='Test')\n","\n","# Standardise training and testing data\n","scaler_train = StandardScaler()\n","scaler_test = StandardScaler()\n","\n","scaler_train.fit(training_data)\n","scaler_test.fit(testing_data)\n","\n","testing_data = scaler_test.transform(testing_data)\n","\n","# Convert training data to pd dataframe\n","columns = \"B N F NIC LI\".split()\n","training_data = pd.DataFrame(data=training_data, index=None, columns=columns)\n","\n","# Replicate the training data\n","replicated_data1 = replicate_data(training_data, 50, 0.03)\n","replicated_data2 = replicate_data(training_data, 50, 0.05)\n","\n","training_data = training_data.append(replicated_data1, ignore_index=True, sort=False)\n","training_data = training_data.append(replicated_data2, ignore_index=True, sort=False)\n","\n","training_data = scaler_train.transform(training_data)\n","training_data = np.array(training_data)\n","\n","# Calculate training and testing labels\n","try:\n","    a = []\n","    for index, row in enumerate(training_data):\n","        dB = training_data[index + 1][0] - row[0]\n","        dN = training_data[index + 1][1] - row[1]\n","        dF = training_data[index + 1][2] - row[2]\n","        dNIC = training_data[index + 1][3] - row[3]        \n","        rates = [dB, dN, dF, dNIC]\n","        a.append(rates)\n","except IndexError:\n","    rates = [0, 0, 0, 0]\n","    a.append(rates)\n","\n","a = np.array(a)\n","training_data = np.append(training_data, a, axis=1)\n","\n","try:\n","    a = []\n","    for index, row in enumerate(testing_data):\n","        dB = testing_data[index + 1][0] - row[0]\n","        dN = testing_data[index + 1][1] - row[1]\n","        dF = testing_data[index + 1][2] - row[2]\n","        dNIC = testing_data[index + 1][3] - row[3]\n","\n","        rates = [dB, dN, dF, dNIC]\n","        a.append(rates)\n","except IndexError:\n","    rates = [0, 0, 0, 0]\n","    a.append(rates)\n","\n","a = np.array(a)\n","testing_data = np.append(testing_data, a, axis=1)\n","\n","# Remove 15th datapoints from all corresponding training and testing sets\n","count = 0\n","decrement = 0\n","for index, row in enumerate(training_data):\n","    count += 1\n","    if count == 15:\n","        delete = index - decrement\n","        training_data = np.delete(training_data, delete, 0)\n","        decrement += 1\n","        count = 0\n","\n","count = 0\n","decrement = 0\n","for index, row in enumerate(testing_data):\n","    count += 1\n","    if count == 15:\n","        delete = index - decrement\n","        testing_data = np.delete(testing_data, delete, 0)\n","        decrement += 1\n","        count = 0\n","\n","HL = 1\n","HN = 17\n","EPOCHS = 260\n","LR = 0.004\n","BATCH_SIZE = 10\n","avg_mse=1\n","\n","xcl_dir = '/content/drive/My Drive/Colab Notebooks/GitHub/MSc/RNN/Results/1HL/' #create a new folder for prediction rsults\n","try:\n","  os.mkdir(xcl_dir)\n","except:\n","  pass\n","\n","for count in range(1):\n","  avg_mse=1\n","  min_mse=1\n","  while count < 50: # this count is related manually to if else for excel saving\n","\n","    rnn = RNN(4, 5, 14, HN, HL)\n","    training_inputs = training_data[:, 0:5]\n","    training_labels = training_data[:, 5:]\n","    test_inputs = testing_data[:, 0:5]\n","    test_labels = testing_data[:, 5:]\n","\n","    training_inputs = np.split(training_inputs, 505)\n","    training_labels = np.split(training_labels, 505)\n","    test_inputs = np.split(test_inputs, 2)\n","    test_labels = np.split(test_labels, 2)\n","\n","    train(rnn, training_inputs, training_labels, EPOCHS, LR, BATCH_SIZE)\n","    avg_mse, predictions_online, predictions_offline = test(test_inputs, test_labels, rnn, BATCH_SIZE)\n","    count = count+1\n","    if min_mse >= avg_mse or count==49: #count=* is related to while count above\n","      min_mse = avg_mse\n","      count_min = count\n","      # Save file every minimum found\n","      predictions_online_inverse_transform = scaler_test.inverse_transform(predictions_online)\n","      predictions_offline_inverse_transform = scaler_test.inverse_transform(predictions_offline)\n","\n","      online = pd.DataFrame(predictions_online_inverse_transform)\n","      offline = pd.DataFrame(predictions_offline_inverse_transform)\n","      avg_mse = pd.DataFrame([avg_mse, 0])\n","      f= round(min_mse.item(), 5)\n","      with pd.ExcelWriter('{xcl_dir}Predictions {f}_{x}_{y}_{a}_{b}_{c}_{count}.xlsx'.format(xcl_dir=xcl_dir, x=HL, y=HN, a=EPOCHS, b=LR, c=BATCH_SIZE, count=count, f=f)) as writer:  \n","          offline.to_excel(writer, sheet_name='Offline', startrow=1, startcol=1)\n","          online.to_excel(writer, sheet_name='Online', startrow=1, startcol=1)\n","          avg_mse.to_excel(writer, sheet_name='Avg_MSE', startrow=1, startcol=1)\n","      torch.save(rnn.state_dict(), '{xcl_dir}Model {f}_{x}_{y}_{a}_{b}_{c}_{count}.pt'.format(xcl_dir=xcl_dir, x=HL, y=HN, z=HN, a=EPOCHS, b=LR, c=BATCH_SIZE, count=count, f=f))\n","    print(avg_mse, min_mse, count, count_min)\n","print(f'\\nDuration: {time.time() - start_time:.0f} seconds')"],"execution_count":null,"outputs":[]}]}