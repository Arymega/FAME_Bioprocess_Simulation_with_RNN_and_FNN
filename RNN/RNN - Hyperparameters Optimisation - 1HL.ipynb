{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"RNN - Hyperparameters Optimisation - 1HL.ipynb","provenance":[{"file_id":"1MRoEq2_1zDfqfhBZ9OorrtImxrIXP9ss","timestamp":1592721766639},{"file_id":"1Zq17yRxUskck6WrqCtDBs2sT3H8cASzz","timestamp":1591904362600},{"file_id":"1GV8hcETq8HS8vPmOaC4Z7fJcR2Hterma","timestamp":1591263473503}],"collapsed_sections":[],"toc_visible":true,"mount_file_id":"1CUmaCIPFcv8pxuH_thBysk9W19NyUN63","authorship_tag":"ABX9TyOWy5S7/esq+fWLj+ayClUX"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"yMzLsNEuuMNu","colab_type":"text"},"source":["# Ray Tune Environment Starter"]},{"cell_type":"code","metadata":{"id":"fxG7EF0BAUFh","colab_type":"code","cellView":"both","colab":{}},"source":["## Dependencies for Google Colab environment \n","\n","\n","print(\"Setting up colab environment\")\n","!pip uninstall -y -q pyarrow\n","!pip install -q https://s3-us-west-2.amazonaws.com/ray-wheels/latest/ray-0.8.0.dev5-cp36-cp36m-manylinux1_x86_64.whl\n","!pip install -q ray[debug]\n","\n","# # A hack to force the runtime to restart, needed to include the above dependencies.\n","print(\"Done installing! Restarting via forced crash (this is not an issue).\")\n","import os\n","os._exit(0)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8ixIH0uhYbIk","colab_type":"text"},"source":["## TensorFLow for Colab"]},{"cell_type":"code","metadata":{"id":"69jc5XEFZ7a4","colab_type":"code","colab":{}},"source":["## TensorFlow 2.0 \n","\n","try:\n","   # %tensorflow_version only exists in Colab.\n","  %tensorflow_version 2.x\n","except Exception:\n","  pass"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6kp_yCKkYgA6","colab_type":"text"},"source":["## Ray Tune Library Import"]},{"cell_type":"code","metadata":{"id":"dBcrMKeo0FzU","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","try:\n","    tf.get_logger().setLevel('INFO')\n","except Exception as exc:\n","    print(exc)\n","import warnings\n","warnings.simplefilter(\"ignore\")\n","\n","import os\n","import numpy as np\n","import torch\n","import torch.optim as optim\n","from torchvision import datasets\n","from ray.tune.examples.mnist_pytorch import get_data_loaders, ConvNet \n","\n","import ray\n","from ray import tune\n","from ray.tune import track\n","from ray.tune.schedulers import PopulationBasedTraining\n","from ray.tune.util import validate_save_restore\n","from ray.tune.schedulers import AsyncHyperBandScheduler\n","\n","%matplotlib inline\n","import matplotlib.style as style\n","import matplotlib.pyplot as plt\n","style.use(\"ggplot\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eS6S_2KFjOCu","colab_type":"text"},"source":["# Function codes"]},{"cell_type":"markdown","metadata":{"id":"9qr7um8qjLR2","colab_type":"text"},"source":["## Test Function"]},{"cell_type":"code","metadata":{"id":"7DiNXvz_jJRM","colab_type":"code","colab":{}},"source":["import torch\n","import numpy as np\n","import pandas as pd\n","from torch.autograd import Variable\n","\n","def test(test_inputs, test_labels, net):\n","    net.eval()\n","    test_X = Variable(torch.Tensor(test_inputs)) \n","    test_y = Variable(torch.Tensor(test_labels))\n","\n","    hidden = net.init_hidden(test_X)\n","    with torch.no_grad():\n","        net_out, _ = net(test_X, hidden)  # Hidden state not required for manual feeding\n","\n","    squared_error_X = []\n","    squared_error_N = []\n","    squared_error_L = [] \n","    squared_error_C = [] \n","\n","    for index1, element in enumerate(test_y):\n","        for index2, row in enumerate(element):\n","            X_error = row[0] - net_out[index1][index2][0]\n","            N_error = row[1] - net_out[index1][index2][1]\n","            L_error = row[2] - net_out[index1][index2][2]\n","            C_error = row[3] - net_out[index1][index2][3]\n","            squared_error_X.append(X_error**2)\n","            squared_error_N.append(N_error**2)\n","            squared_error_L.append(L_error**2)\n","            squared_error_C.append(C_error**2)\n","\n","    MSE_X1 = sum(squared_error_X[0:14])/14\n","    MSE_N1 = sum(squared_error_N[0:14])/14\n","    MSE_L1 = sum(squared_error_L[0:14])/14\n","    MSE_C1 = sum(squared_error_C[0:14])/14\n","    MSE_X2 = sum(squared_error_X[14:28])/14                                     # To accomodate more than 1 test sets \n","    MSE_N2 = sum(squared_error_N[14:28])/14\n","    MSE_L2 = sum(squared_error_L[14:28])/14\n","    MSE_C2 = sum(squared_error_C[14:28])/14\n","    MSE_list = [MSE_X1, MSE_N1, MSE_L1, MSE_C1, MSE_X2, MSE_N2, MSE_L2, MSE_C2,]\n","    AVG_MSE = (sum(MSE_list)/8)\n","\n","    return AVG_MSE\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R6ZNpdCtjHI1","colab_type":"text"},"source":["## Train Function"]},{"cell_type":"code","metadata":{"id":"upghiKsCjFRK","colab_type":"code","colab":{}},"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from tqdm import tqdm\n","from torch.autograd import Variable\n","\n","def train(net, inputs, labels, EPOCHS, l_rate, BATCH_SIZE):\n","    net.train()                                                                         \n","    optimiser = optim.Adam(net.parameters(), lr = l_rate)\t\t\t\t\t\t\t\t\t   # net.parameters(): all of the adjustable parameters in our network. lr: a hyperparameter adjusts the size of the step that the optimizer will take to minimise the loss.\n","    loss_function = nn.MSELoss(reduction='mean')\n","\n","    X = Variable(torch.Tensor(inputs))\n","    y = Variable(torch.Tensor(labels))\n","\n","    for epoch in range(EPOCHS):\n","        for i in tqdm(range(0, len(X), BATCH_SIZE)):\n","            batch_X = X[i:i+BATCH_SIZE]\n","            batch_y = y[i:i+BATCH_SIZE]\n","            hidden = net.init_hidden(batch_X)\n","            optimiser.zero_grad()\n","            outputs, _ = net(batch_X, hidden)\n","            loss = loss_function(outputs, batch_y)\n","            loss.backward()\n","            optimiser.step()\n","  \n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Zt7wZlZ2jCvP","colab_type":"text"},"source":["## Replication **Function**"]},{"cell_type":"code","metadata":{"id":"I2pRV5pEi-F_","colab_type":"code","colab":{}},"source":["import numpy as np\n","import pandas as pd\n","\n","## Create a function which takes in a dataset and replicates it\n","def replicate_data(data, replications, noise):                                                      # Create a function which accepts a dataset and replicates it\n","    cols = list(data.columns) \n","    dataR = data[cols[0:4]]                                                                         # Create a secondary dataframe containing only columns 1-3 (the columns we want to replicate)\n","    df = data                                                                                       # Create the output dataframe that will contain both the original and the replicated data\n","    new_data = pd.DataFrame(columns=data.columns)\n","    i = 0                                                                                           # Initialise replication counter to 0\n","    while i < replications:\n","        replicated_data =  np.random.uniform(dataR-dataR*noise, dataR+dataR*noise)                  # Create random noise for each value in columns 2-4 of dataset\n","        replicated_data = pd.DataFrame(data=replicated_data, index=None, columns=dataR.columns)      # Cast the replicated data as a pandas DataFrame Object\n","        replicated_data['LI'] = df[cols[4]]                                                          # Add the missing light intensity column back into the replicated_data set\n","#        replicated_data['NIC'] = df[cols[4]]                                                         # Add the missing nitrate inflow concentration back into the replicated_data set\n","        new_data = new_data.append(replicated_data, ignore_index=True, sort=False)\n","        i += 1\n","    return new_data\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oDrCOY4Fiqcz","colab_type":"text"},"source":["## RNN Model"]},{"cell_type":"code","metadata":{"id":"WkcLPynaikIW","colab_type":"code","colab":{}},"source":["import torch\n","import torch.nn as nn\n","from torch.autograd import Variable\n","\n","torch.manual_seed(45)\n","\n","### Model ###\n","class RNN(nn.Module):\n","    def __init__(self, num_outputs, input_size, sequence_length, hidden_size, num_layers):\n","        super(RNN, self).__init__()\n","\n","        self.num_outputs = num_outputs\n","        self.input_size = input_size\n","        self.sequence_length = sequence_length\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","\n","        self.rnn = nn.RNN(input_size=self.input_size, hidden_size=self.hidden_size, num_layers=self.num_layers, batch_first=True)\n","        self.fc = nn.Linear(self.hidden_size, self.num_outputs)\n","    \n","    def forward(self, x, hidden):\n","        # Reshape input to (batch_size, sequence_length, input_size)\n","        x = x.view(x.size(0), self.sequence_length, self.input_size)\n","\n","        # Propagate input through RNN\n","        # Input: (batch, seq_len, input_size)\n","        out, _ = self.rnn(x, hidden)\n","        fc_out = self.fc(out)\n","        return fc_out, _\n","    \n","    def init_hidden(self, x):\n","        # Initialse hidden and cell states\n","        # (num_layers * num_directions, batch_size, hidden_size)\n","        return Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size))\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"-doUydZy4_tN"},"source":["# Hyperparameter Tuning"]},{"cell_type":"markdown","metadata":{"id":"a_D3Geouu8m-","colab_type":"text"},"source":["## Data Sets Import"]},{"cell_type":"code","metadata":{"id":"i9iPrwuk5KJ8","colab_type":"code","colab":{}},"source":["# Clone the entire repo.\n","!git clone -l -s git://github.com/Arymega/FAME_Bioprocess_Simulation_with_RNN_and_FNN.git cloned-repo\n","%cd cloned-repo\n","!ls"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UbHRMblvZtli","colab_type":"text"},"source":["## FNN Configuration"]},{"cell_type":"code","metadata":{"id":"_exsrm8FYZkQ","colab_type":"code","colab":{}},"source":["import pandas as pd\n","import numpy as np \n","import copy\n","#from rnn import RNN\n","#from replicate import replicate_data\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import KFold\n","#from train import train\n","#from test2 import test\n","import csv\n","import time\n","start_time = time.time()\n","\n","# Load training data as pd dataframe and convert pd dataframe into numpy array.\n","training_data = pd.read_excel('/content/cloned-repo/Common Files/Datasets.xlsx', sheet_name='6Sets')\n","training_data_array = np.array(training_data)\n","\n","# Standardise Training Data\n","scaler_train = StandardScaler()\n","scaler_train.fit(training_data)\n","\n","# Split data into k=6 folds.\n","kf = KFold(n_splits=6)\n","kf.get_n_splits(training_data)\n","\n","# Split training data set into 6 subsets containing k-1 folds before optimisation.\n","class wrapper(object):\n","    def __init__(self):\n","        self.value = []\n","\n","subset_train1 = wrapper() \n","subset_train2 = wrapper()\n","subset_train3 = wrapper()\n","subset_train4 = wrapper()\n","subset_train5 = wrapper()\n","subset_train6 = wrapper()\n","subset_test1 = wrapper() \n","subset_test2 = wrapper()\n","subset_test3 = wrapper()\n","subset_test4 = wrapper()\n","subset_test5 = wrapper()\n","subset_test6 = wrapper()\n","subset_train_list = [subset_train1, subset_train2, subset_train3, subset_train4, subset_train5, subset_train6]\n","subset_test_list = [subset_test1, subset_test2, subset_test3, subset_test4, subset_test5, subset_test6]\n","\n","index = 0\n","for train_index, test_index in kf.split(training_data):\n","\n","    for row in train_index:\n","        subset_train_list[index].value.append(training_data_array[row])\n","    \n","    for row in test_index:\n","        subset_test_list[index].value.append(training_data_array[row])\n","    \n","    index +=1\n","\n","\n","# Standardise Test Data\n","for subset in subset_test_list:\n","    subset.value = scaler_train.transform(subset.value)\n","\n","# Replicate and Standardise the training data in each subset.\n","\n","columns = \"B N F NIC LI\".split()\n","for index, subset in enumerate(subset_train_list):\n","    df = pd.DataFrame(data=subset.value, index=None, columns=columns)\n","    ref = df\n","    df = scaler_train.transform(df)\n","\n","    replicated_data1 = replicate_data(ref, 50, 0.03)\n","    replicated_data1 = scaler_train.transform(replicated_data1)\n","    df = np.append(df, replicated_data1, axis=0) \n","\n","    replicated_data2 = replicate_data(ref, 50, 0.05)\n","    replicated_data2 = scaler_train.transform(replicated_data2)\n","    df = np.append(df, replicated_data2, axis=0) \n","\n","    subset.value = df\n","\n","\n","# Calculate training and test labels\n","for index1, subset in enumerate(subset_train_list):\n","    a = []\n","    \n","    try:\n","        for index2, row in enumerate(subset.value):\n","            dB = subset.value[index2 + 1][0] - row[0]\n","            dN = subset.value[index2 + 1][1] - row[1]\n","            dF = subset.value[index2 + 1][2] - row[2]\n","            dNIC = subset.value[index2 + 1][3] - row[3]\n","\n","            rates =[dB, dN, dF, dNIC]\n","            a.append(rates)\n","    except IndexError:\n","        rates = [0, 0, 0, 0]\n","        a.append(rates)\n","    \n","    a = np.array(a)\n","    subset.value = np.append(subset.value, a, axis=1) \n","\n","for index1, subset in enumerate(subset_test_list):\n","    b = []\n","    \n","    try:\n","        for index2, row in enumerate(subset.value):\n","            dB = subset.value[index2 + 1][0] - row[0] \n","            dN = subset.value[index2 + 1][1] - row[1]\n","            dF = subset.value[index2 + 1][2] - row[2]\n","            dNIC = subset.value[index2 + 1][3] - row[3]\n","\n","            rates =[dB, dN, dF, dNIC]\n","            b.append(rates)\n","    except IndexError:\n","        rates = [0, 0, 0, 0]\n","        b.append(rates)\n","    \n","    b = np.array(b)\n","    subset.value = np.append(subset.value, b, axis=1)\n","\n","\n","# Remove all 15th datapoints from corresponding training and testing sets\n","for subset in subset_train_list:\n","    count = 0\n","    decrement = 0\n","    for index, row in enumerate(subset.value):\n","        count +=1\n","        if count == 15:\n","            delete = index - decrement\n","            subset.value = np.delete(subset.value, delete, 0)\n","            decrement += 1\n","            count = 0\n","\n","for subset in subset_test_list:\n","    subset.value = np.delete(subset.value, -1, 0)\n","\n","subset_train_list = np.array(subset_train_list)\n","subset_test_list = np.array(subset_test_list)\n","\n","# Shuffle Training Data\n","for subset in subset_train_list:\n","    np.random.shuffle(subset.value)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YNEWIsxcd3xx","colab_type":"text"},"source":["## Train_tune definition\n"]},{"cell_type":"code","metadata":{"id":"xh0o8W1Xd298","colab_type":"code","colab":{}},"source":["# k-fold cross validation training loop\n","HL = 1      #Initial configuration\n","HN = 5  \n","EPOCHS = 15 \n","BATCH_SIZE = 10\n","LR = 0.001\n","MODELS = {}\n","h1 = HN\n","e = EPOCHS\n","momentum = 0.9\n","\n","def train_tune(config):\n","  lr=config[\"LR\"]\n","  h1 = config[\"HN\"]\n","  e=config[\"EPOCHS\"]\n","  rnn = RNN(4, 5, 14, h1, HL)\n","  init_state = copy.deepcopy(rnn.state_dict())\n","  MSEs = []\n","\n","  for count in range(iteration):\n","\n","    for index, subset in enumerate(subset_train_list):\n","\n","      subset.value = np.array(subset.value)\n","      subset_test_list[index].value = np.array(subset_test_list[index].value)\n","      \n","      rnn.load_state_dict(init_state)\n","\n","      training_inputs = subset.value[:, 0:5]\n","      training_labels = subset.value[:, 5:]\n","      test_inputs = subset_test_list[index].value[:, 0:5]\n","      test_labels = subset_test_list[index].value[:, 5:]\n","\n","      training_inputs = np.split(training_inputs, 505)\n","      training_labels = np.split(training_labels, 505)\n","\n","      test_inputs = np.array([test_inputs])\n","      test_labels = np.array([test_labels])\n","      \n","      train(rnn, training_inputs, training_labels, e, LR, BATCH_SIZE)\n","\n","      avg_mse = test(test_inputs, test_labels, rnn)\n","      MSEs.append(avg_mse)\n","\n","    acc = sum(MSEs)/len(MSEs)\n","    tune.track.log(mean_accuracy=acc.item())\n","    MODELS['HL{a}_HN{x}-lr{y}_ep{z}_bs{bs}_ite{count}'.format(a=HL, x=h1, y=lr, z=e, bs=BATCH_SIZE, count=count)] = acc\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6PcIfT0o4IYm","colab_type":"text"},"source":["## Ray Tune Configuration 2"]},{"cell_type":"code","metadata":{"id":"824Fkq8hxgBt","colab_type":"code","colab":{}},"source":["class PytorchTrainble(tune.Trainable):\n","    def _setup(self, config):\n","        self.device = torch.device(\"cpu\")\n","        self.train_loader, self.test_loader = get_data_loaders()\n","        self.model = ConvNet().to(self.device)\n","        self.optimizer = optim.SGD(\n","            self.model.parameters(),\n","            lr=config.get(\"lr\", 0.01),\n","            momentum=config.get(\"momentum\", 0.9))\n","\n","    def _train(self):\n","      rnn = RNN(4, 5, 14, h1, HL)\n","      init_state = copy.deepcopy(rnn.state_dict())\n","      MSEs = []\n","      iteration = 1\n","      for count in range(iteration):\n","\n","        for index, subset in enumerate(subset_train_list):\n","\n","          subset.value = np.array(subset.value)\n","          subset_test_list[index].value = np.array(subset_test_list[index].value)\n","          \n","          rnn.load_state_dict(init_state)\n","\n","          training_inputs = subset.value[:, 0:5]\n","          training_labels = subset.value[:, 5:]\n","          test_inputs = subset_test_list[index].value[:, 0:5]\n","          test_labels = subset_test_list[index].value[:, 5:]\n","\n","          training_inputs = np.split(training_inputs, 505)\n","          training_labels = np.split(training_labels, 505)\n","\n","          test_inputs = np.array([test_inputs])\n","          test_labels = np.array([test_labels])\n","          \n","          train(rnn, training_inputs, training_labels, e, LR, BATCH_SIZE)\n","\n","          avg_mse = test(test_inputs, test_labels, rnn)\n","          MSEs.append(avg_mse)\n","\n","        acc = sum(MSEs)/len(MSEs)\n","\n","        return {\"mean_accuracy\": acc.item()}\n","\n","    def _save(self, checkpoint_dir):\n","        checkpoint_path = os.path.join(checkpoint_dir, \"model.pth\")\n","        torch.save(self.model.state_dict(), checkpoint_path)\n","        return checkpoint_path\n","\n","    def _restore(self, checkpoint_path):\n","        self.model.load_state_dict(torch.load(checkpoint_path))\n","        \n","    def reset_config(self, new_config):\n","        del self.optimizer\n","        self.optimizer = optim.SGD(\n","            self.model.parameters(),\n","            lr=new_config.get(\"lr\", 0.01),\n","            momentum=new_config.get(\"momentum\", 0.9))\n","        return True\n","\n","\n","ray.shutdown()  # Restart Ray defensively in case the ray connection is lost. \n","ray.init(log_to_driver=False)\n","\n","validate_save_restore(PytorchTrainble)\n","validate_save_restore(PytorchTrainble, use_object_store=True)\n","print(\"Success!\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TThK9tdkbajT","colab_type":"text"},"source":["##PBT Configuration"]},{"cell_type":"code","metadata":{"id":"3oShba7qzNms","colab_type":"code","colab":{}},"source":["scheduler = PopulationBasedTraining(\n","    time_attr=\"training_iteration\",\n","    metric=\"mean_accuracy\",\n","    mode=\"min\",\n","    perturbation_interval=5,\n","    hyperparam_mutations={\n","        # distribution for resampling\n","        \"lr\": lambda: np.random.uniform(0.0001, 1),\n","        # allow perturbations within this set of categorical values\n","        \"momentum\": [0.8, 0.9, 0.99],\n","    }\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"31TMPSxi2aX2","colab_type":"code","colab":{}},"source":["ray.shutdown()  # Restart Ray defensively in case the ray connection is lost. \n","ray.init(log_to_driver=False)\n","\n","import time\n","start_time = time.time()\n","\n","analysis = tune.run(\n","    PytorchTrainble,\n","    name=\"pbt_test\",\n","    scheduler=scheduler,\n","    reuse_actors=True,\n","    verbose=1,\n","    stop={\n","        \"training_iteration\": 40,\n","    },\n","    num_samples=40,\n","    \n","    # PBT starts by training many neural networks in parallel with random hyperparameters. \n","    config={\n","        \"lr\": tune.choice([0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008, 0.0009, 0.001, 0.002, 0.003, 0.004, 0.005,]),\n","        \"h1\" : tune.randint(5, 20),\n","        \"e\" : tune.randint(100, 500),\n","        \"momentum\": tune.choice([0.9]),\n","    })\n","\n","print(f'\\nDuration: {time.time() - start_time:.0f} seconds')\n","\n","print('best config:', analysis.get_best_config(\"mean_accuracy\"))\n","\n","# Visualize all mutations of Population-based Training.\n","! cat ~/ray_results/pbt_test/pbt_global.txt\n","\n","# Plot by wall-clock time\n","dfs = analysis.fetch_trial_dataframes()\n","# This plots everything on the same plot\n","ax = None\n","for d in dfs.values():\n","    ax = d.plot(\"training_iteration\", \"mean_accuracy\", figsize=(25,5),ax=ax, legend=False)\n","plt.xlabel(\"epoch\"); plt.ylabel(\"Test Accuracy\"); \n","\n","sav_dir = '/content/drive/My Drive/Colab Notebooks/GitHub/MSc/RNN/Results/1HL/' #create a new folder for prediction rsults\n","zfile = 'ray_results_1HL.zip'\n","try:\n","  os.mkdir(sav_dir)\n","except:\n","  pass\n","!zip -r '{sav_dir}{zfile}' /root/ray_results\n","\n","%load_ext tensorboard\n","%tensorboard --logdir ~/ray_results/pbt_test"],"execution_count":null,"outputs":[]}]}